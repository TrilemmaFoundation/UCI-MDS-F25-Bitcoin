{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f1e62747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import Image, display\n",
    "import plotly.io as pio\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a2334",
   "metadata": {},
   "source": [
    "# 1.2.1 BTC Price and Dynamic DCA Allocation under Non-overlapping 1-year Rolling Windows (2018–2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_price_by_year(\n",
    "    start_year: int,\n",
    "    end_year: int | None = None,\n",
    "    file_path: str = \"data/weight_price.csv\"\n",
    "):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # If only one year is given\n",
    "    if end_year is None:\n",
    "        end_year = start_year + 1\n",
    "\n",
    "    # Window: June 1 -> May 31\n",
    "    start_date = pd.Timestamp(f\"{start_year}-06-01\")\n",
    "    end_date   = pd.Timestamp(f\"{end_year}-05-31\")\n",
    "\n",
    "    # Filter by window\n",
    "    df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
    "\n",
    "    # Plot\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # BTC Price (left axis)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"date\"],\n",
    "            y=df[\"PriceUSD\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"BTC Price\"\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "\n",
    "    # Weight (right axis)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df[\"date\"],\n",
    "            y=df[\"weight\"],\n",
    "            name=\"Weight\",\n",
    "            marker=dict(color=\"red\", opacity=1)\n",
    "        ),\n",
    "        secondary_y=True\n",
    "    )\n",
    "\n",
    "    # X-axis: every June 1st\n",
    "    fig.update_layout(\n",
    "        title=f\"BTC Price and Allocation Weight ({start_year} → {end_year})\",\n",
    "        xaxis=dict(\n",
    "            tickmode=\"linear\",\n",
    "            dtick=\"M12\",\n",
    "            tick0=f\"{start_year}-06-01\"\n",
    "        ),\n",
    "        barmode=\"overlay\"\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(title_text=\"BTC Price (USD)\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Weight\", secondary_y=True)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Convert the figure to a PNG and render it as notebook output (so GitHub can display it)\n",
    "    png_bytes = pio.to_image(\n",
    "        fig,\n",
    "        format=\"png\",\n",
    "        width=1400,    # Image width in pixels (increase for wider output)\n",
    "        height=600,    # Image height in pixels\n",
    "        scale=2         # Resolution multiplier (the most important for clarity)\n",
    "    )\n",
    "\n",
    "    # Display the PNG image inside the notebook\n",
    "    display(Image(png_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48ce01",
   "metadata": {},
   "source": [
    "# 2.1.1 SPD Percentile vs Allocation Weight (Aggregated Across Non-Overlapping 365-Day Windows: 2011–2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentile_vs_weight(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct.csv\",\n",
    "    figsize: tuple = (6, 4),\n",
    "    jitter_std: float = 0.3,\n",
    "    alpha: float = 0.25\n",
    "):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert to numeric and clean\n",
    "    df[\"spd_percentile\"] = pd.to_numeric(df[\"spd_percentile\"], errors=\"coerce\")\n",
    "    df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"spd_percentile\", \"weight\"])\n",
    "\n",
    "    # Add jitter on x-axis\n",
    "    jitter = np.random.normal(0, jitter_std, size=len(df))\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(\n",
    "        df[\"spd_percentile\"] + jitter,\n",
    "        df[\"weight\"],\n",
    "        alpha=alpha\n",
    "    )\n",
    "    plt.xlabel(\"SPD Percentile\")\n",
    "    plt.ylabel(\"Weight\")\n",
    "    plt.title(\"SPD Percentile vs Weight (2011–2025)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_by_spd_bucket(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct.csv\",\n",
    "    bucket_size: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a boxplot of weight distribution by SPD percentile buckets,\n",
    "    and highlight the mean value with a red dot (log scale).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert columns to numeric and remove NaN rows\n",
    "    df[\"spd_percentile\"] = pd.to_numeric(df[\"spd_percentile\"], errors=\"coerce\")\n",
    "    df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"spd_percentile\", \"weight\"])\n",
    "\n",
    "    # Create percentile buckets (e.g. 0–20, 20–40, ...)\n",
    "    bins = list(range(0, 100 + bucket_size, bucket_size))\n",
    "    df[\"bucket\"] = pd.cut(df[\"spd_percentile\"], bins=bins)\n",
    "\n",
    "    # Collect values for each bucket\n",
    "    bucket_groups = []\n",
    "    labels = []\n",
    "\n",
    "    for name, group in df.groupby(\"bucket\", observed=True):\n",
    "        bucket_groups.append(group[\"weight\"].values)\n",
    "        labels.append(str(name))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    bp = plt.boxplot(\n",
    "        bucket_groups,\n",
    "        labels=labels,\n",
    "        showfliers=True,\n",
    "        showmeans=True,\n",
    "        meanline=False\n",
    "    )\n",
    "\n",
    "    # Use log scale for better visibility\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    # Force the mean points to be red and more visible\n",
    "    for m in bp[\"means\"]:\n",
    "        m.set_marker(\"o\")\n",
    "        m.set_markerfacecolor(\"red\")\n",
    "        m.set_markeredgecolor(\"red\")\n",
    "        m.set_markersize(7)\n",
    "\n",
    "    # Add custom legend for the mean\n",
    "    plt.scatter([], [], color=\"red\", label=\"Mean Weight\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"SPD Percentile Bucket\")\n",
    "    plt.ylabel(\"Weight (log scale)\")\n",
    "    plt.title(\"Weight Distribution by SPD Percentile Bucket (Mean Highlighted)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0029f59",
   "metadata": {},
   "source": [
    "# 2.2 Simple Returns Win Rate in a yearly window: Dynamic vs Uniform DCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674933c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_vs_uniform_return_by_window(\n",
    "    file_path: str = \"data/port_dca.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    dyn_col: str = \"dyn_portfolio_value\",\n",
    "    uni_col: str = \"uni_portfolio_value\",\n",
    "    budget: float = 1.0,\n",
    ") -> tuple[float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assumptions\n",
    "    ----------\n",
    "    - Data is already concatenated by rolling windows in order\n",
    "      (e.g. 2018-06-01~2019-06-01, then 2018-06-02~2019-06-02, ...)\n",
    "    - A new window starts when date[t] <= date[t-1]\n",
    "    - Each window uses the SAME budget, and budget is fully invested\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    win_rate, result_df\n",
    "\n",
    "    result_df contains:\n",
    "    - window_id\n",
    "    - start_date\n",
    "    - end_date\n",
    "    - dyn_return\n",
    "    - uni_return\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # identify window boundaries\n",
    "    is_new_window = df[date_col].diff().le(pd.Timedelta(0))\n",
    "    is_new_window.iloc[0] = True\n",
    "    df[\"window_id\"] = is_new_window.cumsum()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for wid, g in df.groupby(\"window_id\", sort=False):\n",
    "\n",
    "        g = g.sort_values(date_col)\n",
    "\n",
    "        start_date = g[date_col].iloc[0]\n",
    "        end_date = g[date_col].iloc[-1]\n",
    "\n",
    "        final_dyn_value = g[dyn_col].iloc[-1]\n",
    "        final_uni_value = g[uni_col].iloc[-1]\n",
    "\n",
    "        if pd.isna(final_dyn_value) or pd.isna(final_uni_value):\n",
    "            continue\n",
    "\n",
    "        dyn_return = (final_dyn_value - budget) / budget\n",
    "        uni_return = (final_uni_value - budget) / budget\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"window_id\": wid,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"dyn_return\": dyn_return,\n",
    "                \"uni_return\": uni_return,\n",
    "                \"dyn_win_flag\": int(dyn_return > uni_return),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    if len(result_df) == 0:\n",
    "        win_rate = np.nan\n",
    "    else:\n",
    "        win_rate = pd.DataFrame(\n",
    "            {\"win_rate_dynamic_%\": [round(result_df[\"dyn_win_flag\"].mean() * 100, 2)]},\n",
    "            index=[\"Annual Return\"]\n",
    "        )\n",
    "    return win_rate, result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa8cf",
   "metadata": {},
   "source": [
    "# 2.4 Investment Efficiency: Dynamic vs Uniform DCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dynamic_vs_uniform_efficiency(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    dyn_weight_col: str = \"weight\",\n",
    "    spd_pct_col: str = \"spd_percentile\",\n",
    "    budget: float = 10_000.0,\n",
    "    cheap_pct_threshold: float = 30.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare Dynamic DCA vs Uniform DCA efficiency.\n",
    "\n",
    "    Metrics included:\n",
    "    - Total BTC Accumulation\n",
    "    - Weighted Average SPD (sats per dollar)\n",
    "    - Effective Average Purchase Price ($/BTC)\n",
    "    - Timing Efficiency (% of capital invested in the cheapest X% of SPD)\n",
    "\n",
    "    Required columns in the CSV:\n",
    "    - date (optional but recommended)\n",
    "    - price               -> BTC price\n",
    "    - dynamic_weight      -> Dynamic DCA daily weight\n",
    "    - spd_percentile      -> SPD percentile (0–100)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and sort data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df = df.sort_values(date_col)\n",
    "\n",
    "    price = df[price_col].astype(float).values\n",
    "    dyn_w_raw = df[dyn_weight_col].astype(float).values\n",
    "    spd_pct = df[spd_pct_col].astype(float).values\n",
    "\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"DataFrame is empty.\")\n",
    "\n",
    "    # Normalize weights\n",
    "    # Dynamic weights may not sum to 1 → normalize\n",
    "    dyn_w = dyn_w_raw / dyn_w_raw.sum()\n",
    "\n",
    "    # Uniform DCA: equal allocation every day\n",
    "    uni_w = np.full(n, 1.0 / n)\n",
    "\n",
    "    # Compute SPD (satoshis per dollar)\n",
    "    spd = 100_000_000.0 / price  # sats per $1\n",
    "\n",
    "    # Daily allocation & BTC purchased\n",
    "    dyn_alloc = dyn_w * budget\n",
    "    uni_alloc = uni_w * budget\n",
    "\n",
    "    dyn_btc = dyn_alloc / price\n",
    "    uni_btc = uni_alloc / price\n",
    "\n",
    "    dyn_total_btc = dyn_btc.sum()\n",
    "    uni_total_btc = uni_btc.sum()\n",
    "\n",
    "    # Weighted Average SPD\n",
    "    dyn_wspd = np.average(spd, weights=dyn_alloc)\n",
    "    uni_wspd = np.average(spd, weights=uni_alloc)\n",
    "\n",
    "    # Effective Average Purchase Price\n",
    "    dyn_eff_price = budget / dyn_total_btc\n",
    "    uni_eff_price = budget / uni_total_btc\n",
    "\n",
    "    # Timing Efficiency\n",
    "    # % of capital invested in the cheapest X% of prices (SPD percentile)\n",
    "    cheap_mask = spd_pct >= cheap_pct_threshold\n",
    "\n",
    "    dyn_timing = dyn_alloc[cheap_mask].sum() / dyn_alloc.sum()\n",
    "    uni_timing = uni_alloc[cheap_mask].sum() / uni_alloc.sum()\n",
    "\n",
    "    dyn_timing_pct = dyn_timing * 100.0\n",
    "    uni_timing_pct = uni_timing * 100.0\n",
    "\n",
    "    # Summary table\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"Dynamic DCA\": {\n",
    "                \"Total BTC Accumulation\": round(dyn_total_btc, 1),\n",
    "                \"Weighted Avg SPD (sats per $)\": round(dyn_wspd, 1),\n",
    "                \"Effective Avg Purchase Price ($/BTC)\": round(dyn_eff_price, 1),\n",
    "                f\"Timing Efficiency (% capital in top {int(cheap_pct_threshold)}%)\": round(dyn_timing_pct, 1),\n",
    "            },\n",
    "            \"Uniform DCA\": {\n",
    "                \"Total BTC Accumulation\": round(uni_total_btc, 1),\n",
    "                \"Weighted Avg SPD (sats per $)\": round(uni_wspd, 1),\n",
    "                \"Effective Avg Purchase Price ($/BTC)\": round(uni_eff_price, 1),\n",
    "                f\"Timing Efficiency (% capital in top {int(cheap_pct_threshold)}%)\": round(uni_timing_pct, 1),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddedce65",
   "metadata": {},
   "source": [
    "# 3.1.1 Risk Profile Comparison: Dynamic vs Uniform DCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk metric helper funcs\n",
    "def sharpe_from_returns(returns: pd.Series, trading_days: int = 252) -> float:\n",
    "    \"\"\"Compute annualized Sharpe ratio from a daily series (here: BTC-based g_t).\"\"\"\n",
    "    r = returns.dropna()\n",
    "    if len(r) < 30:\n",
    "        return np.nan\n",
    "\n",
    "    mean_daily = r.mean()\n",
    "    std_daily = r.std(ddof=1)\n",
    "    if std_daily == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return (mean_daily / std_daily) * np.sqrt(trading_days)\n",
    "\n",
    "# Risk metric helper funcs\n",
    "def volatility_from_returns(returns: pd.Series, trading_days: int = 252) -> float:\n",
    "    \"\"\"Compute annualized volatility from a daily series (here: BTC-based g_t).\"\"\"\n",
    "    r = returns.dropna()\n",
    "    if len(r) < 2:\n",
    "        return np.nan\n",
    "    return r.std(ddof=1) * np.sqrt(trading_days)\n",
    "\n",
    "# Rolling 12m risk metrics by pre-cut windows=\n",
    "def rolling_12m_risk_metrics_by_window(\n",
    "    file_path: str = \"data/port_dca.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    dyn_col: str = \"dyn_btc\",\n",
    "    uni_col: str = \"uni_btc\",\n",
    "    trading_days: int = 252,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use BTC cumulative amount to compute g_t,\n",
    "    then compute Sharpe / Volatility\n",
    "\n",
    "    Assumptions:\n",
    "    - port_dca.csv contains multiple 12-month windows concatenated sequentially\n",
    "    - When the date is no longer strictly increasing (date[t] <= date[t-1]),\n",
    "      it is treated as the start of a new window\n",
    "    - For each window:\n",
    "        * Use \"cumulative BTC amount\" to compute daily g_t:\n",
    "              g_t = BTC_t - BTC_{t-1}\n",
    "        * Then compute from g_t:\n",
    "            - Sharpe\n",
    "            - Volatility\n",
    "        * Also mark whether Dynamic “beats” Uniform on each metric:\n",
    "            - Sharpe: higher is better\n",
    "            - Volatility: lower is better\n",
    "            - Max Drawdown: higher (less negative) is better\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Detect the start of a new window when the date is no longer strictly increasing\n",
    "    is_new_window = df[date_col].diff().le(pd.Timedelta(0))\n",
    "    is_new_window.iloc[0] = True\n",
    "    df[\"window_id\"] = is_new_window.cumsum()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for wid, g in df.groupby(\"window_id\", sort=False):\n",
    "        g = g.sort_values(date_col)\n",
    "\n",
    "        start_date = g[date_col].iloc[0]\n",
    "        end_date   = g[date_col].iloc[-1]\n",
    "\n",
    "        dyn_btc = g[dyn_col]\n",
    "        uni_btc = g[uni_col]\n",
    "\n",
    "        # g_t = BTC_t - BTC_{t-1}\n",
    "        dyn_g = dyn_btc.diff()\n",
    "        uni_g = uni_btc.diff()\n",
    "\n",
    "        # Sharpe\n",
    "        dyn_sharpe  = sharpe_from_returns(dyn_g, trading_days=trading_days)\n",
    "        uni_sharpe  = sharpe_from_returns(uni_g, trading_days=trading_days)\n",
    "\n",
    "        dyn_vol     = volatility_from_returns(dyn_g, trading_days=trading_days)\n",
    "        uni_vol     = volatility_from_returns(uni_g, trading_days=trading_days)\n",
    "\n",
    "        # Win/Loss logic: higher Sharpe is better\n",
    "        sharpe_win   = int(dyn_sharpe  > uni_sharpe)\n",
    "\n",
    "        # Lower volatility is better\n",
    "        vol_win      = int(dyn_vol < uni_vol)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"window_id\": wid,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"dyn_sharpe\": dyn_sharpe,\n",
    "                \"uni_sharpe\": uni_sharpe,\n",
    "                \"dyn_volatility\": dyn_vol,\n",
    "                \"uni_volatility\": uni_vol,\n",
    "                \"dyn_sharpe_win\": sharpe_win,\n",
    "                \"dyn_vol_win\": vol_win,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "\n",
    "    # Win rate table: Dynamic DCA vs Uniform for each risk metric\n",
    "    if len(metrics_df) == 0:\n",
    "        win_rate_table = pd.DataFrame(\n",
    "            {\"dynamic dca win rate\": [np.nan, np.nan]},\n",
    "            index=[\"sharpe\", \"volatility_annual\"],\n",
    "        )\n",
    "    else:\n",
    "        sharpe_wr  = metrics_df[\"dyn_sharpe_win\"].mean()\n",
    "        vol_wr     = metrics_df[\"dyn_vol_win\"].mean()\n",
    "\n",
    "        win_rate_table = pd.DataFrame(\n",
    "            {\n",
    "                \"win_rate_dynamic_%\": [\n",
    "                    round(sharpe_wr * 100, 2),\n",
    "                    round(vol_wr * 100, 2),\n",
    "                ]\n",
    "            },\n",
    "            index=[\"sharpe\", \"volatility_annual\"],\n",
    "        )\n",
    "\n",
    "    return metrics_df, win_rate_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_portfolio_and_risk_metrics_full_history(\n",
    "    file_path: str = \"data/weight_price.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    weight_col: str = \"weight\",\n",
    "    annual_budget: float = 10_000.0,\n",
    "    start_month: int = 6,\n",
    "    start_day: int = 1,\n",
    "    trading_days: int = 252,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # 1. Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    price = df[price_col].astype(float)\n",
    "\n",
    "    # 2. Assign cycle year\n",
    "    years = df[date_col].dt.year\n",
    "    months = df[date_col].dt.month\n",
    "    days = df[date_col].dt.day\n",
    "\n",
    "    cycle_year = years.copy()\n",
    "    before_cycle_start = (months < start_month) | (\n",
    "        (months == start_month) & (days < start_day)\n",
    "    )\n",
    "    cycle_year[before_cycle_start] -= 1\n",
    "    df[\"cycle_year\"] = cycle_year\n",
    "\n",
    "    # 3. Dynamic weights\n",
    "    raw_w = df[weight_col].astype(float)\n",
    "    cycle_sum = df.groupby(\"cycle_year\")[weight_col].transform(\"sum\")\n",
    "\n",
    "    df[\"dyn_norm_weight\"] = np.where(cycle_sum > 0, raw_w / cycle_sum, 0.0)\n",
    "    df[\"dyn_invest\"] = df[\"dyn_norm_weight\"] * annual_budget\n",
    "\n",
    "    # 4. Uniform weights\n",
    "    cycle_count = df.groupby(\"cycle_year\")[\"cycle_year\"].transform(\"count\")\n",
    "    df[\"uni_norm_weight\"] = np.where(cycle_count > 0, 1.0 / cycle_count, 0.0)\n",
    "    df[\"uni_invest\"] = df[\"uni_norm_weight\"] * annual_budget\n",
    "\n",
    "    # 5. BTC bought & held\n",
    "    df[\"dyn_btc_bought\"] = df[\"dyn_invest\"] / price\n",
    "    df[\"uni_btc_bought\"] = df[\"uni_invest\"] / price\n",
    "\n",
    "    df[\"dyn_btc_held\"] = df[\"dyn_btc_bought\"].cumsum()\n",
    "    df[\"uni_btc_held\"] = df[\"uni_btc_bought\"].cumsum()\n",
    "\n",
    "    # 6. Portfolio value\n",
    "    df[\"dyn_portfolio_value\"] = df[\"dyn_btc_held\"] * price\n",
    "    df[\"uni_portfolio_value\"] = df[\"uni_btc_held\"] * price\n",
    "\n",
    "    # 7. BTC-change based returns (Scheme B)\n",
    "    df[\"dyn_g\"] = df[\"dyn_btc_held\"].diff()\n",
    "    df[\"uni_g\"] = df[\"uni_btc_held\"].diff()\n",
    "\n",
    "    # 8. Risk metrics\n",
    "    def compute_risk_metrics_btc(btc_diff: pd.Series, portfolio_value: pd.Series):\n",
    "\n",
    "        r = btc_diff.dropna()\n",
    "\n",
    "        if len(r) < 30:\n",
    "            return {\n",
    "                \"sharpe\": np.nan,\n",
    "                \"volatility_annual\": np.nan,\n",
    "            }\n",
    "\n",
    "        mean_daily = r.mean()\n",
    "        std_daily = r.std(ddof=1)\n",
    "\n",
    "        # Sharpe\n",
    "        sharpe = np.nan\n",
    "        if std_daily > 0:\n",
    "            sharpe = (mean_daily / std_daily) * np.sqrt(trading_days)\n",
    "\n",
    "        # Volatility\n",
    "        volatility_annual = std_daily * np.sqrt(trading_days)\n",
    "\n",
    "        return {\n",
    "            \"sharpe\": round(sharpe, 4) if pd.notna(sharpe) else np.nan,\n",
    "            \"volatility_annual\": round(volatility_annual, 4),\n",
    "        }\n",
    "\n",
    "    # 9. Compute both strategies\n",
    "    dyn_metrics = compute_risk_metrics_btc(\n",
    "        df[\"dyn_g\"], df[\"dyn_portfolio_value\"]\n",
    "    )\n",
    "\n",
    "    uni_metrics = compute_risk_metrics_btc(\n",
    "        df[\"uni_g\"], df[\"uni_portfolio_value\"]\n",
    "    )\n",
    "\n",
    "    metrics_df = pd.DataFrame(\n",
    "        [dyn_metrics, uni_metrics],\n",
    "        index=[\"Dynamic DCA\", \"Uniform DCA\"],\n",
    "    )\n",
    "\n",
    "    return df, metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ef434",
   "metadata": {},
   "source": [
    "# 3.1.2 Why Dynamic DCA Performs Worse on Sharpe/Volatility Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14748a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_from_returns(returns: pd.Series, trading_days: int = 252) -> float:\n",
    "    r = returns.dropna()\n",
    "    if len(r) < 30:\n",
    "        return np.nan\n",
    "    return r.mean() * np.sqrt(trading_days)\n",
    "\n",
    "def volatility_from_returns(returns: pd.Series, trading_days: int = 252) -> float:\n",
    "    r = returns.dropna()\n",
    "    if len(r) < 2:\n",
    "        return np.nan\n",
    "    return r.std(ddof=1) * np.sqrt(trading_days)\n",
    "\n",
    "def rolling_12m_mean_vol_returns_by_window(\n",
    "    file_path: str = \"data/port_dca.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    dyn_col: str = \"dyn_btc\",\n",
    "    uni_col: str = \"uni_btc\",\n",
    "    trading_days: int = 252,\n",
    "):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Detect the start of a new window when the date is no longer strictly increasing\n",
    "    is_new_window = df[date_col].diff().le(pd.Timedelta(0))\n",
    "    is_new_window.iloc[0] = True\n",
    "    df[\"window_id\"] = is_new_window.cumsum()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for wid, g in df.groupby(\"window_id\", sort=False):\n",
    "        g = g.sort_values(date_col)\n",
    "\n",
    "        start_date = g[date_col].iloc[0]\n",
    "        end_date   = g[date_col].iloc[-1]\n",
    "\n",
    "        dyn_btc = g[dyn_col]\n",
    "        uni_btc = g[uni_col]\n",
    "\n",
    "        # g_t = BTC_t - BTC_{t-1}\n",
    "        dyn_g = dyn_btc.diff()\n",
    "        uni_g = uni_btc.diff()\n",
    "\n",
    "        # Returns\n",
    "        dyn_mean  = mean_from_returns(dyn_g, trading_days=trading_days)\n",
    "        uni_mean  = mean_from_returns(uni_g, trading_days=trading_days)\n",
    "\n",
    "        dyn_vol  = volatility_from_returns(dyn_g, trading_days=trading_days)\n",
    "        uni_vol  = volatility_from_returns(uni_g, trading_days=trading_days)\n",
    "\n",
    "        mean_win   = int(dyn_mean  > uni_mean)\n",
    "        std_win   = int(dyn_vol < uni_vol)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"window_id\": wid,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"dyn_mean\": dyn_mean,\n",
    "                \"uni_mean\": uni_mean,\n",
    "                \"dyn_vol\": dyn_vol,\n",
    "                \"uni_vol\": uni_vol,\n",
    "                \"dyn_mean_win\": mean_win,\n",
    "                \"dyn_vol_win\": std_win,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "\n",
    "    # Win rate table: Dynamic DCA vs Uniform for each risk metric\n",
    "    if len(metrics_df) == 0:\n",
    "        win_rate_table = pd.DataFrame(\n",
    "            {\"dynamic dca win rate\": [np.nan, np.nan, np.nan, np.nan]},\n",
    "            index=[\"return_annual\", \"volatility_annual\"],\n",
    "        )\n",
    "    else:\n",
    "        mean_wr  = metrics_df[\"dyn_mean_win\"].mean()\n",
    "        vol_wr     = metrics_df[\"dyn_vol_win\"].mean()\n",
    "\n",
    "        win_rate_table = pd.DataFrame(\n",
    "            {\n",
    "                \"win_rate_dynamic_%\": [\n",
    "                    round(mean_wr * 100, 2),\n",
    "                    round(vol_wr * 100, 2),\n",
    "                ]\n",
    "            },\n",
    "            index=[\"return_annual\", \"volatility_annual\"],\n",
    "        )\n",
    "\n",
    "    return metrics_df, win_rate_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7634eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gt_distribution_with_tail_plotly(\n",
    "    window_id: int,\n",
    "    file_path: str = \"data/port_dca.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    dyn_col: str = \"dyn_btc\",\n",
    "    uni_col: str = \"uni_btc\",\n",
    "    window_col: str = \"window_id\",\n",
    "    start_col: str = \"start_date\",\n",
    "    end_col: str = \"end_date\",\n",
    "    bins: int = 40,\n",
    "    tail_q: float = 0.9,   # Quantile threshold for defining the “tail” (e.g. 0.9 = top 10%)\n",
    "):\n",
    "    \"\"\"\n",
    "    Row 1:\n",
    "      X-axis: Daily allocation g_t (in sats) = (BTC_t - BTC_{t-1}) * 1e8\n",
    "      LEFT  y-axis: Frequency (Days in window)\n",
    "      RIGHT y-axis: Probability (count / total days)\n",
    "\n",
    "    Row 2:\n",
    "      Shows only the right tail (x >= tail_quantile) frequency,\n",
    "      so the extreme Dynamic values are more clearly visible.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Parse dates\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    for c in [start_col, end_col]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c])\n",
    "\n",
    "    # Filter one window\n",
    "    win_df = df[df[window_col] == window_id].copy()\n",
    "    if win_df.empty:\n",
    "        raise ValueError(f\"No rows found for window_id = {window_id}\")\n",
    "\n",
    "    win_df = win_df.sort_values(date_col)\n",
    "\n",
    "    # Title suffix\n",
    "    if start_col in win_df.columns and end_col in win_df.columns:\n",
    "        start_date = win_df[start_col].iloc[0].date()\n",
    "        end_date = win_df[end_col].iloc[0].date()\n",
    "        title_suffix = f\"{start_date} → {end_date}\"\n",
    "    else:\n",
    "        title_suffix = f\"window_id = {window_id}\"\n",
    "\n",
    "    # ----- 1) Cumulative BTC → daily g_t (BTC) → convert to sats -----\n",
    "    dyn_cum = win_df[dyn_col].dropna().reset_index(drop=True)\n",
    "    uni_cum = win_df[uni_col].dropna().reset_index(drop=True)\n",
    "\n",
    "    dyn_g_btc = dyn_cum.diff().dropna()\n",
    "    uni_g_btc = uni_cum.diff().dropna()\n",
    "\n",
    "    dyn_sats = dyn_g_btc * 1e8\n",
    "    uni_sats = uni_g_btc * 1e8\n",
    "\n",
    "    all_vals = np.concatenate([dyn_sats.values, uni_sats.values])\n",
    "\n",
    "    # ----- 2) Full-range bins + probability -----\n",
    "    bin_edges = np.linspace(all_vals.min(), all_vals.max(), bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "    N_dyn = len(dyn_sats)\n",
    "    N_uni = len(uni_sats)\n",
    "\n",
    "    counts_dyn, _ = np.histogram(dyn_sats, bins=bin_edges)\n",
    "    counts_uni, _ = np.histogram(uni_sats, bins=bin_edges)\n",
    "\n",
    "    prob_dyn = counts_dyn / N_dyn\n",
    "    prob_uni = counts_uni / N_uni\n",
    "\n",
    "    dyn_mean = dyn_sats.mean()\n",
    "    uni_mean = uni_sats.mean()\n",
    "\n",
    "    # ----- 3) Define tail region -----\n",
    "    tail_start = np.quantile(all_vals, tail_q)\n",
    "\n",
    "    dyn_tail = dyn_sats[dyn_sats >= tail_start]\n",
    "    uni_tail = uni_sats[uni_sats >= tail_start]\n",
    "\n",
    "    if len(dyn_tail) == 0 and len(uni_tail) == 0:\n",
    "        print(\"No tail values above the chosen quantile, try lowering tail_q.\")\n",
    "        tail_bins = 10\n",
    "    else:\n",
    "        tail_bins = max(6, bins // 3)\n",
    "\n",
    "    tail_edges = np.linspace(tail_start, all_vals.max(), tail_bins)\n",
    "    tail_centers = (tail_edges[:-1] + tail_edges[1:]) / 2\n",
    "\n",
    "    counts_dyn_tail, _ = np.histogram(dyn_tail, bins=tail_edges)\n",
    "    counts_uni_tail, _ = np.histogram(uni_tail, bins=tail_edges)\n",
    "\n",
    "    # ----- 4) Plot: two-row subplot -----\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=False,\n",
    "        specs=[[{\"secondary_y\": True}], [{\"secondary_y\": False}]],\n",
    "        row_heights=[0.7, 0.3],\n",
    "        vertical_spacing=0.12,\n",
    "    )\n",
    "\n",
    "    # ===== Row 1: Full distribution (same logic as before, now in sats) =====\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=dyn_sats,\n",
    "            xbins=dict(start=bin_edges[0], end=bin_edges[-1], size=bin_width),\n",
    "            name=\"Dynamic gₜ (count)\",\n",
    "            opacity=0.5,\n",
    "        ),\n",
    "        row=1, col=1, secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=uni_sats,\n",
    "            xbins=dict(start=bin_edges[0], end=bin_edges[-1], size=bin_width),\n",
    "            name=\"Uniform gₜ (count)\",\n",
    "            opacity=0.5,\n",
    "        ),\n",
    "        row=1, col=1, secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bin_centers,\n",
    "            y=prob_dyn,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Dynamic prob.\",\n",
    "        ),\n",
    "        row=1, col=1, secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bin_centers,\n",
    "            y=prob_uni,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Uniform prob.\",\n",
    "        ),\n",
    "        row=1, col=1, secondary_y=True,\n",
    "    )\n",
    "\n",
    "    # Mean lines (full range)\n",
    "    fig.add_vline(\n",
    "        x=dyn_mean,\n",
    "        line_width=2,\n",
    "        line_dash=\"dash\",\n",
    "        row=1, col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_vline(\n",
    "        x=uni_mean,\n",
    "        line_width=2,\n",
    "        line_dash=\"dash\",\n",
    "        row=1, col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_annotation(\n",
    "        x=dyn_mean,\n",
    "        y=0.9,\n",
    "        yref=\"paper\",\n",
    "        text=\"Dynamic Mean\",\n",
    "        showarrow=False,\n",
    "        xanchor=\"left\",\n",
    "    )\n",
    "\n",
    "    fig.add_annotation(\n",
    "        x=uni_mean,\n",
    "        y=0.83,\n",
    "        yref=\"paper\",\n",
    "        text=\"Uniform Mean\",\n",
    "        showarrow=False,\n",
    "        xanchor=\"left\",\n",
    "    )\n",
    "\n",
    "    # ===== Row 2: Tail zoom-in =====\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=dyn_tail,\n",
    "            xbins=dict(start=tail_edges[0], end=tail_edges[-1], size=tail_edges[1] - tail_edges[0]),\n",
    "            name=f\"Dynamic gₜ tail (>{int(tail_q*100)}% quantile)\",\n",
    "            opacity=0.6,\n",
    "        ),\n",
    "        row=2, col=1, secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=uni_tail,\n",
    "            xbins=dict(start=tail_edges[0], end=tail_edges[-1], size=tail_edges[1] - tail_edges[0]),\n",
    "            name=f\"Uniform gₜ tail (>{int(tail_q*100)}% quantile)\",\n",
    "            opacity=0.6,\n",
    "        ),\n",
    "        row=2, col=1, secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"Daily Allocation gₜ (sats per day)\",\n",
    "        row=2, col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"Frequency (Days in window)\",\n",
    "        row=1, col=1, secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"Probability\",\n",
    "        row=1, col=1, secondary_y=True,\n",
    "        rangemode=\"tozero\",\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=f\"Tail Frequency (x ≥ {int(tail_q*100)}% quantile)\",\n",
    "        row=2, col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=(\n",
    "            \"Daily BTC Allocation Distribution (1-Year Window)<br>\"\n",
    "            f\"{title_suffix} (gₜ measured in sats; bottom panel zooms in on the right tail)\"\n",
    "        ),\n",
    "        barmode=\"overlay\",\n",
    "        legend_title_text=\"Strategy\",\n",
    "        width=950,\n",
    "        height=650,\n",
    "        margin=dict(l=70, r=60, t=90, b=70),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Convert the figure to a PNG and render it as notebook output (so GitHub can display it)\n",
    "    png_bytes = pio.to_image(\n",
    "        fig,\n",
    "        format=\"png\",\n",
    "        width=950,    # Image width in pixels (increase for wider output)\n",
    "        height=650,    # Image height in pixels\n",
    "        scale=1         # Resolution multiplier (the most important for clarity)\n",
    "    )\n",
    "\n",
    "    # Display the PNG image inside the notebook\n",
    "    display(Image(png_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d3d1f",
   "metadata": {},
   "source": [
    "# 3.2 Allocation Response to Risk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_weight_after_80pct(\n",
    "    file_path: str = \"data/port_dca_spd.csv\",\n",
    "    pct_col: str = \"spd_pct\",\n",
    "    weight_col: str = \"weight\",\n",
    "    window_col: str = \"window_id\",\n",
    "    threshold: float = 80.0\n",
    "):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Select rows within each window where SPD percentile >= threshold (e.g., 80th)\n",
    "    filtered = df[df[pct_col] >= threshold]\n",
    "\n",
    "    # For each window, sum the weights of all days above the percentile threshold\n",
    "    sum_per_window = filtered.groupby(window_col)[weight_col].sum()\n",
    "\n",
    "    # Compute the average of these weight sums across all windows\n",
    "    avg_weight = round(sum_per_window.mean(), 4)\n",
    "\n",
    "    # Print the final result\n",
    "    print(\"Average weight sum after 80th percentile = {} %\".format(avg_weight * 100))\n",
    "\n",
    "    return avg_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0090009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_price_across_year(\n",
    "    start_year: int,\n",
    "    end_year: int | None = None,\n",
    "    file_path: str = \"data/weight_price.csv\"\n",
    "):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # If only one year is given\n",
    "    if end_year is None:\n",
    "        end_year = start_year + 1\n",
    "\n",
    "    # Window: June 1 → May 31\n",
    "    start_date = pd.Timestamp(f\"{start_year}-06-01\")\n",
    "    end_date   = pd.Timestamp(f\"{end_year}-05-31\")\n",
    "\n",
    "    # Filter rows\n",
    "    df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
    "\n",
    "    # Plot: create figure with secondary y-axis\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # BTC price\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"date\"],\n",
    "            y=df[\"PriceUSD\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"BTC Price\",\n",
    "            line=dict(width=1.3),\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "\n",
    "    # Weight bars (more visible)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df[\"date\"],\n",
    "            y=df[\"weight\"],\n",
    "            name=\"Weight\",\n",
    "            marker=dict(\n",
    "                color=\"#CC0000\",     # deeper red\n",
    "                opacity=0.4,           # fully solid\n",
    "                line=dict(color=\"red\", width=0.5)  # thicker outline\n",
    "            ),\n",
    "            width=3 * 24 * 60 * 60 * 1000  # make bars wider (3-day width in ms)\n",
    "        ),\n",
    "        secondary_y=True\n",
    "    )\n",
    "\n",
    "    # Add June 1 lines for each rolling window break\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        june_1 = pd.Timestamp(f\"{year}-06-01\")\n",
    "        if start_date <= june_1 <= end_date:\n",
    "            fig.add_vline(\n",
    "                x=june_1.to_pydatetime(),\n",
    "                line_width=2,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\"\n",
    "            )\n",
    "            fig.add_annotation(\n",
    "                x=june_1.to_pydatetime(),\n",
    "                y=1,\n",
    "                yref=\"paper\",\n",
    "                text=f\"{year}-06-01<br>window\",\n",
    "                showarrow=False,\n",
    "                font=dict(size=11)\n",
    "            )\n",
    "\n",
    "    # Build yearly ticks manually (no dtick!)\n",
    "    tick_vals = [pd.Timestamp(f\"{y}-06-01\") for y in range(start_year, end_year + 1)]\n",
    "    tick_text = [f\"{y}-06-01\" for y in range(start_year, end_year + 1)]\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"BTC Price and Allocation Weight ({start_year} → {end_year})\",\n",
    "        xaxis=dict(\n",
    "            tickmode=\"array\",\n",
    "            tickvals=tick_vals,\n",
    "            ticktext=tick_text\n",
    "        ),\n",
    "        barmode=\"overlay\",\n",
    "        width=1100,\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(title_text=\"BTC Price (USD)\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Weight\", secondary_y=True)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Convert the figure to a PNG and render it as notebook output (so GitHub can display it)\n",
    "    png_bytes = pio.to_image(\n",
    "        fig,\n",
    "        format=\"png\",\n",
    "        width=1000,    # Image width in pixels (increase for wider output)\n",
    "        height=500,    # Image height in pixels\n",
    "        scale=2         # Resolution multiplier (the most important for clarity)\n",
    "    )\n",
    "\n",
    "    # Display the PNG image inside the notebook\n",
    "    display(Image(png_bytes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4ab70",
   "metadata": {},
   "source": [
    "# 4.1.1 Major historical cycles with bearish and bullish phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd682c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_vs_ma200(\n",
    "    file_path: str = \"data/weight_price_ma200.csv\",\n",
    "    start_year: int = 2018,\n",
    "    end_year: int = 2025,\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    ma_col: str = \"MA200\",\n",
    "    date_col: str = \"date\",\n",
    "    buffer: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot BTC historical price with MA200 (2018–2025 by default):\n",
    "\n",
    "    - Single price line, but color-coded by regime relative to MA200:\n",
    "        * Bullish   (green):   Price > MA200 * (1 + buffer)\n",
    "        * Sideways  (gray):    MA200 * (1 - buffer) <= Price <= MA200 * (1 + buffer)\n",
    "        * Bearish   (red):     Price < MA200 * (1 - buffer)\n",
    "    - One dashed MA200 line (dark blue).\n",
    "    - Also exports a PNG so GitHub can display the chart.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.io as pio\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Time filter\n",
    "    df = df[\n",
    "        (df[date_col] >= f\"{start_year}-01-01\") &\n",
    "        (df[date_col] <= f\"{end_year}-12-31\")\n",
    "    ].copy()\n",
    "\n",
    "    # --- Define regimes relative to MA200 with ±buffer ---\n",
    "    upper = df[ma_col] * (1 + buffer)\n",
    "    lower = df[ma_col] * (1 - buffer)\n",
    "\n",
    "    df[\"regime\"] = \"sideways\"\n",
    "    df.loc[df[price_col] > upper, \"regime\"] = \"bull\"\n",
    "    df.loc[df[price_col] < lower, \"regime\"] = \"bear\"\n",
    "\n",
    "    # --- Build segments for color-changing price line ---\n",
    "    segments = []\n",
    "    current_regime = df[\"regime\"].iloc[0]\n",
    "    temp_rows = [df.iloc[0]]\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        row = df.iloc[i]\n",
    "        if row[\"regime\"] != current_regime:\n",
    "            segments.append((pd.DataFrame(temp_rows), current_regime))\n",
    "            temp_rows = [row]\n",
    "            current_regime = row[\"regime\"]\n",
    "        else:\n",
    "            temp_rows.append(row)\n",
    "\n",
    "    # last segment\n",
    "    segments.append((pd.DataFrame(temp_rows), current_regime))\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Regime → color / legend name\n",
    "    color_map = {\n",
    "        \"bull\": \"green\",\n",
    "        \"sideways\": \"gray\",\n",
    "        \"bear\": \"red\",\n",
    "    }\n",
    "    name_map = {\n",
    "        \"bull\": f\"Price > {ma_col} × (1 + {buffer:.1f})\",\n",
    "        \"sideways\": f\"|Price − {ma_col}| ≤ {int(buffer*100)}%\",\n",
    "        \"bear\": f\"Price < {ma_col} × (1 − {buffer:.1f})\",\n",
    "    }\n",
    "\n",
    "    # Only show each legend item once\n",
    "    legend_shown = {\"bull\": False, \"sideways\": False, \"bear\": False}\n",
    "\n",
    "    # --- Add colored price segments ---\n",
    "    for seg_df, regime in segments:\n",
    "        color = color_map[regime]\n",
    "        name = name_map[regime]\n",
    "\n",
    "        show = not legend_shown[regime]\n",
    "        legend_shown[regime] = True\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=seg_df[date_col],\n",
    "                y=seg_df[price_col],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=color, width=2),\n",
    "                name=name,\n",
    "                showlegend=bool(show),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- MA200 dashed line (more visible color) ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[date_col],\n",
    "            y=df[ma_col],\n",
    "            mode=\"lines\",\n",
    "            name=ma_col,\n",
    "            line=dict(\n",
    "                width=3,\n",
    "                dash=\"dash\",\n",
    "                color=\"darkblue\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        title=f\"BTC Price vs {ma_col} with Regimes (buffer = {int(buffer*100)}%) \"\n",
    "              f\"({start_year}–{end_year})\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Price (USD)\",\n",
    "        width=1000,\n",
    "        height=500,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0),\n",
    "    )\n",
    "\n",
    "    # Interactive output in notebook\n",
    "    fig.show()\n",
    "\n",
    "    # Convert the figure to a PNG and render it as notebook output (GitHub can display this)\n",
    "    png_bytes = pio.to_image(\n",
    "        fig,\n",
    "        format=\"png\",\n",
    "        width=1000,   # image width in pixels\n",
    "        height=500,   # image height in pixels\n",
    "        scale=2,      # resolution multiplier\n",
    "    )\n",
    "\n",
    "    display(Image(png_bytes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b435ba1",
   "metadata": {},
   "source": [
    "# 4.1.2 Duration of bull/bear regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "447863aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_regime_frequency_two_classes(\n",
    "    file_path=\"data/weight_price_ma200.csv\",\n",
    "    price_col=\"PriceUSD\",\n",
    "    ma_col=\"MA200\",\n",
    "    date_col=\"date\",\n",
    "    buffer=0.2\n",
    "):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    total = len(df)\n",
    "\n",
    "    upper = df[ma_col] * (1 + buffer)   # Bullish\n",
    "    lower = df[ma_col] * (1 - buffer)   # Bearish\n",
    "\n",
    "    df[\"regime\"] = None\n",
    "    df.loc[df[price_col] > upper, \"regime\"] = \"bullish\"\n",
    "    df.loc[df[price_col] < lower, \"regime\"] = \"bearish\"\n",
    "\n",
    "    counts = df[\"regime\"].value_counts()\n",
    "\n",
    "    freq_table = pd.DataFrame({\n",
    "        \"Regime\": [\"bullish\", \"bearish\"],\n",
    "        \"Count\": [counts.get(\"bullish\", 0), counts.get(\"bearish\", 0)]\n",
    "    })\n",
    "\n",
    "    freq_table[\"Percentage %\"] = ((freq_table[\"Count\"] / total) * 100).round(2)\n",
    "\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5a948",
   "metadata": {},
   "source": [
    "# 4.1.3 Dynamic DCA allocation behavior by market regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b894cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_allocation_by_regime(\n",
    "    file_path: str = \"data/weight_price_ma200.csv\",\n",
    "    buffer: float = 0.2,\n",
    ") -> pd.DataFrame:\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    price = df[\"PriceUSD\"]\n",
    "    ma = df[\"MA200\"]\n",
    "\n",
    "    # Define regimes based on Price vs MA20 with a buffer\n",
    "    conditions = [\n",
    "        price > ma * (1 + buffer),  # clearly above MA200 → bull\n",
    "        price < ma * (1 - buffer),  # clearly below MA200 → bear\n",
    "    ]\n",
    "    choices = [\"bull\", \"bear\"]\n",
    "\n",
    "    df[\"regime\"] = np.select(conditions, choices, default=\"sideways\")\n",
    "\n",
    "    # Mean allocation weight per regime\n",
    "    regime_alloc = df.groupby(\"regime\", as_index=False)[\"weight\"].mean()\n",
    "    regime_alloc.rename(columns={\"weight\": \"average_daily_weight\"}, inplace=True)\n",
    "\n",
    "\n",
    "    # ----- Add color rule -----\n",
    "    colors = regime_alloc[\"regime\"].map(\n",
    "        lambda r: \"red\" if r == \"bear\" else \"steelblue\"\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(regime_alloc[\"regime\"], regime_alloc[\"average_daily_weight\"], color=colors)\n",
    "    plt.xlabel(\"Market Regime (based on MA200)\")\n",
    "    plt.ylabel(\"Mean Allocation Weight\")\n",
    "    plt.title(\"Mean Daily Allocation by Market Regime\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return regime_alloc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7b92f",
   "metadata": {},
   "source": [
    "# 4.2.1 Visualize MSTR’s historical buy points on the Bitcoin price chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc007d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mstr_buy_points(\n",
    "    file_path: str = \"data/mstr_price.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    mstr_btc_col: str = \"mstr_btc\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot BTC price with enhanced styling,\n",
    "    and mark MSTR buy points with highlighted red markers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # MSTR buy days\n",
    "    df_buy = df[df[mstr_btc_col] > 0]\n",
    "\n",
    "    # Figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # --- BTC price line ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[date_col],\n",
    "            y=df[price_col],\n",
    "            mode=\"lines\",\n",
    "            name=\"BTC Price\",\n",
    "            line=dict(color=\"steelblue\", width=2.5),\n",
    "            opacity=0.9,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- MSTR buy points ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_buy[date_col],\n",
    "            y=df_buy[price_col],\n",
    "            mode=\"markers\",\n",
    "            name=\"MSTR Buy\",\n",
    "            marker=dict(\n",
    "                color=\"white\",\n",
    "                size=10,\n",
    "                line=dict(color=\"red\", width=2),\n",
    "                symbol=\"circle\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Styling / decorations ---\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"BTC Price with MSTR Buy Points\",\n",
    "            x=0.5,\n",
    "            font=dict(size=22)\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=500,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "\n",
    "        # Background\n",
    "        plot_bgcolor=\"rgba(245, 245, 245, 1)\",\n",
    "        paper_bgcolor=\"white\",\n",
    "\n",
    "        # Axes\n",
    "        xaxis=dict(\n",
    "            title=\"Date\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=False\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"BTC Price (USD)\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=False\n",
    "        ),\n",
    "        margin=dict(l=60, r=40, t=80, b=50),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Convert the figure to a PNG and render it as notebook output (so GitHub can display it)\n",
    "    png_bytes = pio.to_image(\n",
    "        fig,\n",
    "        format=\"png\",\n",
    "        width=1000,    # Image width in pixels (increase for wider output)\n",
    "        height=500,    # Image height in pixels\n",
    "        scale=2         # Resolution multiplier (the most important for clarity)\n",
    "    )\n",
    "\n",
    "    # Display the PNG image inside the notebook\n",
    "    display(Image(png_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d8054",
   "metadata": {},
   "source": [
    "# 4.2.2 Does MSTR really buy low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "403a7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data helper\n",
    "def load_mstr_price(\n",
    "    file_path: str = \"data/mstr_price.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    mstr_btc_col: str = \"mstr_btc\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load MSTR-BTC dataset with columns:\n",
    "        - date\n",
    "        - PriceUSD\n",
    "        - mstr_btc\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Forward-looking SPD percentile analysis\n",
    "def forward_spd_percentiles(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    mstr_btc_col: str = \"mstr_btc\",\n",
    "    horizons: list[int] = [7, 30, 90, 180],\n",
    "    min_days_ratio: float = 0.8,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each MSTR buy event, compute the SPD percentile of the event-day price\n",
    "    within the forward-looking window [t0, t0 + horizon].\n",
    "\n",
    "    Here we use the SPD-percentile formula:\n",
    "\n",
    "        SPD = 1 / PriceUSD   (scaled factor cancels out)\n",
    "        best_SPD  = 1 / min(window_prices)   # lowest price in window\n",
    "        worst_SPD = 1 / max(window_prices)   # highest price in window\n",
    "\n",
    "        spd_pct = (your_SPD - worst_SPD) / (best_SPD - worst_SPD) * 100\n",
    "\n",
    "    Edge handling (Option B):\n",
    "        - For long horizons (e.g., 180d) near the end of the dataset,\n",
    "          the forward window may be truncated.\n",
    "        - We only keep the percentile if the window has at least\n",
    "              min_days_ratio * horizon\n",
    "          points (after dropping NaNs). Otherwise, we set it to NaN.\n",
    "\n",
    "        Example:\n",
    "            horizon = 180, min_days_ratio = 0.8\n",
    "            => need at least 144 data points in [t0, t0 + 180d]\n",
    "               to compute pct_180d; otherwise pct_180d = NaN.\n",
    "\n",
    "    Returns a DataFrame with one row per event:\n",
    "        event_date, price_at_event, {pct_7d, pct_30d, pct_90d, pct_180d, ...}\n",
    "        where pct_*d are SPD percentiles in [0, 100].\n",
    "    \"\"\"\n",
    "    df = df.copy().sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    # MSTR buy events (mstr_btc > 0)\n",
    "    events = df[df[mstr_btc_col] > 0].copy()\n",
    "\n",
    "    records = []\n",
    "    for _, row in events.iterrows():\n",
    "        t0 = row[date_col]\n",
    "        p0 = row[price_col]\n",
    "\n",
    "        rec = {\n",
    "            \"event_date\": t0,\n",
    "            \"price_at_event\": p0\n",
    "        }\n",
    "\n",
    "        # Skip if event price is missing\n",
    "        if pd.isna(p0):\n",
    "            for h in horizons:\n",
    "                rec[f\"pct_{h}d\"] = np.nan\n",
    "            records.append(rec)\n",
    "            continue\n",
    "\n",
    "        # SPD at event date (constant scale dropped)\n",
    "        spd_0 = 1.0 / p0\n",
    "\n",
    "        for h in horizons:\n",
    "            t_end = t0 + pd.Timedelta(days=h)\n",
    "            mask = (df[date_col] >= t0) & (df[date_col] <= t_end)\n",
    "            window_prices = df.loc[mask, price_col].dropna()\n",
    "\n",
    "            # Minimum number of points required for this horizon\n",
    "            min_points = int(np.ceil(h * min_days_ratio))\n",
    "\n",
    "            if len(window_prices) < min_points:\n",
    "                rec[f\"pct_{h}d\"] = np.nan\n",
    "                continue\n",
    "\n",
    "            # Compute SPD for best/worst in the window\n",
    "            p_min = window_prices.min()\n",
    "            p_max = window_prices.max()\n",
    "\n",
    "            # Guard against zero or degenerate window\n",
    "            if p_min <= 0 or p_max <= 0 or p_min == p_max:\n",
    "                rec[f\"pct_{h}d\"] = np.nan\n",
    "                continue\n",
    "\n",
    "            spd_best = 1.0 / p_min   # lowest price -> highest SPD\n",
    "            spd_worst = 1.0 / p_max  # highest price -> lowest SPD\n",
    "\n",
    "            denom = spd_best - spd_worst\n",
    "            if denom == 0:\n",
    "                rec[f\"pct_{h}d\"] = np.nan\n",
    "            else:\n",
    "                spd_pct = (spd_0 - spd_worst) / denom * 100.0\n",
    "                rec[f\"pct_{h}d\"] = spd_pct\n",
    "\n",
    "        records.append(rec)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def summarize_forward_percentiles(\n",
    "    pct_df: pd.DataFrame,\n",
    "    horizons: list[int] = [7, 30, 90, 180]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate forward-looking SPD percentiles across all events.\n",
    "\n",
    "    Input:\n",
    "        pct_df: output of forward_price_percentiles(...)\n",
    "    Output:\n",
    "        DataFrame with one row per horizon:\n",
    "            horizon_days, mean_percentile %, n_events\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for h in horizons:\n",
    "        col = f\"pct_{h}d\"\n",
    "        if col not in pct_df.columns:\n",
    "            continue\n",
    "\n",
    "        vals = pct_df[col].dropna().values\n",
    "        if len(vals) == 0:\n",
    "            rows.append({\n",
    "                \"horizon_days\": h,\n",
    "                \"mean_percentile\": np.nan,\n",
    "                \"n_events\": 0,\n",
    "            })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"horizon_days\": h,\n",
    "                \"mean_percentile\": round(float(np.mean(vals)), 2),\n",
    "                \"n_events\": len(vals),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9aaeda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random timing comparison (using forward-looking SPD percentiles)\n",
    "def random_timing_forward_percentiles(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    mstr_btc_col: str = \"mstr_btc\",\n",
    "    horizons: list[int] = [7, 30, 90, 180],\n",
    "    n_iter: int = 1000,\n",
    "    random_state: int | None = 42,\n",
    "    min_days_ratio: float = 0.8,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Monte Carlo random timing test (SPD-percentile-based).\n",
    "\n",
    "    Idea:\n",
    "        - We want to know: if an investor picks buy dates at random\n",
    "          (same number of buys as MSTR), how good are their entry\n",
    "          prices compared to future prices in terms of SPD percentile.\n",
    "\n",
    "    For each iteration:\n",
    "        1. Randomly pick N dates (N = # of MSTR buy events),\n",
    "           from all dates with non-missing prices.\n",
    "        2. For each sampled \"event\" date t0 and each horizon h:\n",
    "               - Consider the forward window [t0, t0 + h]\n",
    "               - Drop NaN prices\n",
    "               - Require at least min_days_ratio * h points, otherwise skip\n",
    "               - Compute SPD-based percentile using:\n",
    "\n",
    "                     SPD = 1 / price\n",
    "                     best_SPD  = 1 / min(window_prices)   # lowest price\n",
    "                     worst_SPD = 1 / max(window_prices)   # highest price\n",
    "\n",
    "                     spd_pct = (SPD_t0 - worst_SPD) / (best_SPD - worst_SPD) * 100\n",
    "\n",
    "        3. For each horizon, store the average SPD percentile across all\n",
    "           valid sampled events in this iteration.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame of shape [n_iter x len(horizons)] with columns:\n",
    "            mean_pct_7d, mean_pct_30d, mean_pct_90d, mean_pct_180d, ...\n",
    "        Each row = ONE random-timing experiment.\n",
    "        Each value = the average forward-looking SPD percentile for that experiment.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Sort by date to ensure proper time ordering\n",
    "    base = df.copy().sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    # Real MSTR events: only used to match number of buys (N)\n",
    "    real_events = base[base[mstr_btc_col] > 0][date_col].values\n",
    "    n_events = len(real_events)\n",
    "    if n_events == 0:\n",
    "        raise ValueError(\"No MSTR buy events (mstr_btc > 0) found.\")\n",
    "\n",
    "    # Candidate dates: all dates with non-missing prices\n",
    "    candidate_df = base.dropna(subset=[price_col]).reset_index(drop=True)\n",
    "    candidate_dates = candidate_df[date_col].values\n",
    "\n",
    "    if len(candidate_dates) < n_events:\n",
    "        raise ValueError(\"Not enough candidate dates to sample random events.\")\n",
    "\n",
    "    # Use date as index for fast lookup / slicing\n",
    "    df_idx = base.set_index(date_col)\n",
    "\n",
    "    records: list[dict[str, float]] = []\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        # Sample N random \"buy dates\" (no replacement)\n",
    "        sampled_dates = rng.choice(candidate_dates, size=n_events, replace=False)\n",
    "        horizon_means: dict[str, float] = {}\n",
    "\n",
    "        for h in horizons:\n",
    "            pcts: list[float] = []\n",
    "            min_points = int(np.ceil(h * min_days_ratio))\n",
    "\n",
    "            for t0 in sampled_dates:\n",
    "                t0 = pd.Timestamp(t0)\n",
    "                t_end = t0 + pd.Timedelta(days=h)\n",
    "\n",
    "                if t0 not in df_idx.index:\n",
    "                    continue\n",
    "\n",
    "                # Forward window [t0, t0 + h]\n",
    "                window_prices = df_idx.loc[\n",
    "                    (df_idx.index >= t0) & (df_idx.index <= t_end),\n",
    "                    price_col\n",
    "                ].dropna()\n",
    "\n",
    "                # window too short => skip this event for this horizon\n",
    "                if len(window_prices) < min_points:\n",
    "                    continue\n",
    "\n",
    "                p0 = df_idx.loc[t0, price_col]\n",
    "                if pd.isna(p0) or p0 <= 0:\n",
    "                    continue\n",
    "\n",
    "                # SPD at event date\n",
    "                spd_0 = 1.0 / p0\n",
    "\n",
    "                # Best & worst SPD in the window\n",
    "                p_min = window_prices.min()\n",
    "                p_max = window_prices.max()\n",
    "                if p_min <= 0 or p_max <= 0 or p_min == p_max:\n",
    "                    continue\n",
    "\n",
    "                spd_best = 1.0 / p_min   # lowest price -> highest SPD\n",
    "                spd_worst = 1.0 / p_max  # highest price -> lowest SPD\n",
    "\n",
    "                denom = spd_best - spd_worst\n",
    "                if denom == 0:\n",
    "                    continue\n",
    "\n",
    "                spd_pct = (spd_0 - spd_worst) / denom * 100.0\n",
    "                pcts.append(spd_pct)\n",
    "\n",
    "            # Average SPD percentile across all valid sampled events (this iteration)\n",
    "            if len(pcts) == 0:\n",
    "                horizon_means[f\"mean_pct_{h}d\"] = np.nan\n",
    "            else:\n",
    "                horizon_means[f\"mean_pct_{h}d\"] = float(np.mean(pcts))\n",
    "\n",
    "        records.append(horizon_means)\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentile_distributions(\n",
    "    rand_df: pd.DataFrame,\n",
    "    mstr_summary: pd.DataFrame,\n",
    "    horizons: list[int] = [7, 30, 90, 180]\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw distribution plots of random timing percentiles\n",
    "    and overlay MSTR's average percentile as a vertical line.\n",
    "    \"\"\"\n",
    "\n",
    "    # 2x2 subplot\n",
    "    fig = sp.make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[f\"{h}-Day Forward Percentile\" for h in horizons]\n",
    "    )\n",
    "\n",
    "    positions = [(1,1), (1,2), (2,1), (2,2)]\n",
    "\n",
    "    for idx, (h, pos) in enumerate(zip(horizons, positions), start=1):\n",
    "        row, col = pos\n",
    "        colname = f\"mean_pct_{h}d\"\n",
    "\n",
    "        # histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=rand_df[colname],\n",
    "                nbinsx=40,\n",
    "                opacity=0.75\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "        # MSTR value\n",
    "        mstr_val = mstr_summary.loc[mstr_summary[\"horizon_days\"] == h, \"mean_percentile\"].iloc[0]\n",
    "\n",
    "        # vertical line\n",
    "        fig.add_vline(\n",
    "            x=mstr_val,\n",
    "            line_color=\"red\",\n",
    "            line_width=3,\n",
    "            line_dash=\"dash\",\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "        fig.add_annotation(\n",
    "            x=mstr_val,\n",
    "            xref=f\"x{idx}\",\n",
    "            yref=f\"y{idx}\",\n",
    "            text=f\"MSTR = {mstr_val:.3f}\",\n",
    "            showarrow=False,\n",
    "            font=dict(color=\"red\", size=12),\n",
    "            yanchor=\"bottom\",\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        width=700,\n",
    "        title=\"Forward SPD Percentile Distribution vs MSTR Percentile\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Convert the figure to a PNG and render it as notebook output (so GitHub can display it)\n",
    "    png_bytes = pio.to_image(\n",
    "        fig,\n",
    "        format=\"png\",\n",
    "        height=500,\n",
    "        width=700,\n",
    "        scale=1\n",
    "    )\n",
    "\n",
    "    # Display the PNG image inside the notebook\n",
    "    display(Image(png_bytes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_ttest_mstr_rand(\n",
    "    rand_df, mstr_summary, horizons=[7, 30, 90, 180]\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    for h in horizons:\n",
    "        col = f\"mean_pct_{h}d\"\n",
    "        rand_vals = rand_df[col].dropna().values\n",
    "        \n",
    "        rand_mean = rand_vals.mean()\n",
    "        rand_std = rand_vals.std(ddof=1)\n",
    "\n",
    "        mstr_mean = mstr_summary.loc[\n",
    "            mstr_summary.horizon_days == h,\n",
    "            \"mean_percentile\"\n",
    "        ].iloc[0]\n",
    "\n",
    "        # t = (MSTR - Random) / std_random\n",
    "        t_stat = (mstr_mean - rand_mean) / rand_std\n",
    "\n",
    "        # one-sided test: H1: mstr_mean < rand_mean  (left-side)\n",
    "        df = len(rand_vals) - 1\n",
    "        p_value = t.cdf(t_stat, df=df)   # left-tail\n",
    "\n",
    "        rows.append({\n",
    "            \"horizon_days\": h,\n",
    "            \"mstr_mean\": mstr_mean,\n",
    "            \"rand_mean\": rand_mean,\n",
    "            \"std_rand\": rand_std,\n",
    "            \"t_stat\": t_stat,\n",
    "            \"p_value_one_sided\": p_value\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed060cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
