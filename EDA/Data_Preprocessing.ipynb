{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28501146",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2cad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import requests\n",
    "from io import StringIO\n",
    "import io\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132550c",
   "metadata": {},
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bfc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back-test date range\n",
    "BACKTEST_START     = '2011-06-01' \n",
    "BACKTEST_END       = '2025-06-01' \n",
    "\n",
    "# Rolling window length (in months)\n",
    "INVESTMENT_WINDOW  = 12\n",
    "\n",
    "# Step frequency for window start-dates: 'Daily', 'Weekly' or 'Monthly'\n",
    "PURCHASE_FREQ      = 'Daily'\n",
    "\n",
    "# Minimum per-period weight (to avoid zero allocations)\n",
    "MIN_WEIGHT         = 1e-5\n",
    "\n",
    "PURCHASE_FREQ_TO_OFFSET = {\n",
    "    'Daily':   '1D',\n",
    "    'Weekly':  '7D',\n",
    "    'Monthly': '1M',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad558dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7ae80",
   "metadata": {},
   "source": [
    "# Download BTC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cfa6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from coinmetrics.api_client import CoinMetricsClient\n",
    "except ImportError:\n",
    "    raise ImportError(\"coinmetrics.api_client module is required. Install it via pip:\\n\\n    pip install coinmetrics-api-client\")\n",
    "\n",
    "def extract_btc_data_to_csv(local_path='data/btc_data.csv'):\n",
    "    # Coin Metrics BTC CSV (raw GitHub URL)\n",
    "    url = \"https://raw.githubusercontent.com/coinmetrics/data/master/csv/btc.csv\"\n",
    "    \n",
    "    # Download the content\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # raises an error for bad responses\n",
    "    \n",
    "    # Parse CSV content\n",
    "    btc_df = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "    btc_df['time'] = pd.to_datetime(btc_df['time']).dt.normalize()\n",
    "    btc_df['time'] = btc_df['time'].dt.tz_localize(None)\n",
    "    btc_df.set_index('time', inplace=True)\n",
    "\n",
    "    btc_df.to_csv(local_path)\n",
    "    \n",
    "    # Show the df\n",
    "    btc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be90aa",
   "metadata": {},
   "source": [
    "# Load BTC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88568ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(\"data/btc_data.csv\", index_col=0, parse_dates=True)\n",
    "    df = df.loc[~df.index.duplicated(keep='last')]\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "def validate_price_data(df):\n",
    "    if df.empty or 'PriceUSD' not in df.columns:\n",
    "        raise ValueError(\"Invalid BTC price data.\")\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Index must be datetime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976d566",
   "metadata": {},
   "source": [
    "# DCA Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238b2ca",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed54491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FULL_FEATURES = None\n",
    "\n",
    "MIN_W = 1e-5\n",
    "WINS = [30, 90, 180, 365, 1461]\n",
    "FEATS = [f\"z{w}\" for w in WINS]\n",
    "PROTOS = [(0.5, 5.0), (1.0, 1.0), (5.0, 0.5)]\n",
    "\n",
    "# Optimized theta parameters from the final model run\n",
    "THETA = np.array([\n",
    "    1.3507, 1.073, -1.226, 2.5141, 2.9946, -0.4083, -0.1082, -0.6809,\n",
    "    0.3465, -0.6804, -2.9974, -2.9991, -1.2658, -0.368, 0.7567, -1.9627,\n",
    "    -1.9124, 2.9983, 0.5704, 0.0, 0.8669, 1.2546, 5.0\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d35454",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2137345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts a vector of scores into a probability distribution.\"\"\"\n",
    "    ex = np.exp(x - x.max())\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def allocate_sequential(raw: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Strict left-to-right 'drain' allocator.\"\"\"\n",
    "    n = len(raw)\n",
    "    floor = n * MIN_W\n",
    "    rem_budget, rem_raw = 1 - floor, raw.sum()\n",
    "    w = np.empty_like(raw)\n",
    "    for i, x in enumerate(raw):\n",
    "        share = 0 if rem_raw == 0 else (x / rem_raw) * rem_budget\n",
    "        w[i] = MIN_W + share\n",
    "        rem_budget -= share\n",
    "        rem_raw -= x\n",
    "    return w / w.sum()\n",
    "\n",
    "def beta_mix_pdf(n: int, mix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Generates a smooth baseline curve from a mixture of Beta distributions.\"\"\"\n",
    "    t = np.linspace(0.5 / n, 1 - 0.5 / n, n)\n",
    "    return (mix[0] * beta.pdf(t, *PROTOS[0]) +\n",
    "            mix[1] * beta.pdf(t, *PROTOS[1]) +\n",
    "            mix[2] * beta.pdf(t, *PROTOS[2])) / n\n",
    "\n",
    "def zscore(s: pd.Series, win: int) -> pd.Series:\n",
    "    \"\"\"Calculates the rolling z-score for a given series and window.\"\"\"\n",
    "    m = s.rolling(win, win // 2).mean()\n",
    "    sd = s.rolling(win, win // 2).std()\n",
    "    return ((s - m) / sd).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f308b",
   "metadata": {},
   "source": [
    "## Main DCA Strategy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cad4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates features on the full historical data ONCE and returns the\n",
    "    relevant slice. This robustly handles calls from different boilerplate\n",
    "    functions and avoids boundary errors that cause leakage.\n",
    "    \"\"\"\n",
    "    global _FULL_FEATURES\n",
    "\n",
    "    # Only compute the full feature set if it hasn't been done yet.\n",
    "    if _FULL_FEATURES is None:\n",
    "        try:\n",
    "            # Assumes 'btc_data.csv' is in the same directory.\n",
    "            full_price_df = pd.read_csv(\"data/btc_data.csv\", index_col=0, parse_dates=True)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"data/btc_data.csv not found. Please ensure it's in the correct directory.\")\n",
    "        \n",
    "        # Select only the PriceUSD column before doing anything else.\n",
    "        full_price_df = full_price_df[['PriceUSD']]\n",
    "        \n",
    "        # We need history from before the backtest start date for rolling windows.\n",
    "        full_price_df = full_price_df.loc[\"2010-07-18\":]\n",
    "        # full_price_df = full_price_df.loc[\"2010-07-18\":] \n",
    "\n",
    "        log_prices = np.log(full_price_df['PriceUSD'])\n",
    "        \n",
    "        z_all = pd.DataFrame({f\"z{w}\": zscore(log_prices, w).clip(-4, 4) for w in WINS}, index=log_prices.index)\n",
    "        \n",
    "        # The strategy uses lagged features to avoid look-ahead bias.\n",
    "        z_lag = z_all.shift(1).fillna(0)\n",
    "        \n",
    "        _FULL_FEATURES = full_price_df.join(z_lag)\n",
    "\n",
    "    # Return the portion of the pre-computed features that matches the input index.\n",
    "    return _FULL_FEATURES.reindex(df.index).fillna(0)\n",
    "\n",
    "\n",
    "def compute_weights(df_window: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given a slice of data, computes portfolio weights that sum to 1.\n",
    "    This function first calls construct_features to ensure the necessary\n",
    "    feature columns are present.\n",
    "    \"\"\"\n",
    "    if df_window.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    feat_slice = construct_features(df_window)\n",
    "\n",
    "    alpha, beta_v = THETA[:18].reshape(3, 6), THETA[18:]\n",
    "    \n",
    "    # Use features from the first day to set the annual strategy\n",
    "    first_day_feats = feat_slice[FEATS].iloc[0].values\n",
    "    mix = softmax(alpha @ np.r_[1, first_day_feats])\n",
    "    \n",
    "    # Calculate the components of the allocation formula\n",
    "    n_days = len(feat_slice)\n",
    "    base_alloc = beta_mix_pdf(n_days, mix)\n",
    "    dynamic_signal = np.exp(-(feat_slice[FEATS].values @ beta_v))\n",
    "    \n",
    "    # Combine signals and compute final weights\n",
    "    raw_weights = base_alloc * dynamic_signal\n",
    "    final_weights = allocate_sequential(raw_weights)\n",
    "    \n",
    "    return pd.Series(final_weights, index=feat_slice.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb0565",
   "metadata": {},
   "source": [
    "# Run DCA Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8401810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_window_label(window_start: pd.Timestamp, window_end: pd.Timestamp) -> str:\n",
    "    \"\"\"\n",
    "    Format \"YYYY-MM-DD → YYYY-MM-DD\" for a rolling window.\n",
    "    \"\"\"\n",
    "    start_str = pd.to_datetime(window_start).strftime(\"%Y-%m-%d\")\n",
    "    end_str   = pd.to_datetime(window_end).strftime(\"%Y-%m-%d\")\n",
    "    return f\"{start_str} → {end_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd9f5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cycle_spd(\n",
    "    dataframe: pd.DataFrame,\n",
    "    strategy_function\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute sats‐per‐dollar (SPD) stats over rolling windows.\n",
    "\n",
    "    - Uses full‐history features (no look‐ahead).\n",
    "    - Window length = INVESTMENT_WINDOW months.\n",
    "    - Step every PURCHASE_FREQ.\n",
    "    - Returns a DataFrame indexed by window label, with:\n",
    "        min_sats_per_dollar, max_sats_per_dollar,\n",
    "        uniform_sats_per_dollar, dynamic_sats_per_dollar,\n",
    "        uniform_percentile, dynamic_percentile, excess_percentile.\n",
    "    \"\"\"\n",
    "    # 1) Precompute full-history features & restrict to backtest\n",
    "    full_feat = construct_features(dataframe).loc[BACKTEST_START:BACKTEST_END]\n",
    "\n",
    "    # 2) Window parameters\n",
    "    window_offset  = pd.DateOffset(months=INVESTMENT_WINDOW)\n",
    "    step_freq      = PURCHASE_FREQ_TO_OFFSET[PURCHASE_FREQ]\n",
    "\n",
    "    results = []\n",
    "    for window_start in pd.date_range(\n",
    "        start=pd.to_datetime(BACKTEST_START),\n",
    "        end=pd.to_datetime(BACKTEST_END) - window_offset,\n",
    "        freq=step_freq\n",
    "    ):\n",
    "        window_end  = window_start + window_offset\n",
    "        feat_slice  = full_feat.loc[window_start:window_end]\n",
    "        price_slice = dataframe[\"PriceUSD\"].loc[window_start:window_end]\n",
    "\n",
    "        if price_slice.empty:\n",
    "            continue\n",
    "\n",
    "        label       = _make_window_label(window_start, window_end)\n",
    "        inv_price   = (1.0 / price_slice) * 1e8  # sats per dollar\n",
    "\n",
    "        # Compute weights on this slice\n",
    "        weight_slice = strategy_function(feat_slice)\n",
    "\n",
    "        # Uniform vs. dynamic SPD\n",
    "        uniform_spd = inv_price.mean()\n",
    "        dynamic_spd = (weight_slice * inv_price).sum()\n",
    "\n",
    "        # Min/max for percentile scaling\n",
    "        min_spd = inv_price.min()   # low price → high SPD\n",
    "        max_spd = inv_price.max()   # high price → low SPD\n",
    "        span    = max_spd - min_spd\n",
    "\n",
    "        uniform_pct = (uniform_spd - min_spd) / span * 100\n",
    "        dynamic_pct = (dynamic_spd - min_spd) / span * 100\n",
    "\n",
    "        results.append({\n",
    "            \"window\":                   label,\n",
    "            \"min_sats_per_dollar\":      min_spd,\n",
    "            \"max_sats_per_dollar\":      max_spd,\n",
    "            \"uniform_sats_per_dollar\":  uniform_spd,\n",
    "            \"dynamic_sats_per_dollar\":  dynamic_spd,\n",
    "            \"uniform_percentile\":       uniform_pct,\n",
    "            \"dynamic_percentile\":       dynamic_pct,\n",
    "            \"excess_percentile\":        dynamic_pct - uniform_pct,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).set_index(\"window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db1a64",
   "metadata": {},
   "source": [
    "# Other Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41865c7",
   "metadata": {},
   "source": [
    "## Extract Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fa50cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight(\n",
    "    dataframe: pd.DataFrame,\n",
    "    strategy_function,\n",
    "    backtest_start,\n",
    "    backtest_end,\n",
    "    step: str = \"year\",   # \"year\" → non-overlapping, \"day\" → 1-day rolling overlapping\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    step = \"year\":\n",
    "        - window length: INVESTMENT_WINDOW months\n",
    "        - move forward by INVESTMENT_WINDOW months each time → non-overlapping yearly windows\n",
    "\n",
    "    step = \"day\":\n",
    "        - window length: INVESTMENT_WINDOW months\n",
    "        - move forward by 1 day each time → 1-day rolling overlapping windows\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Convert to Timestamp (to avoid errors when subtracting Timedelta from strings)\n",
    "    bt_start = pd.to_datetime(backtest_start)\n",
    "    bt_end   = pd.to_datetime(backtest_end)\n",
    "\n",
    "    # 2) Precompute features and restrict to the backtest period\n",
    "    full_feat = construct_features(dataframe).loc[bt_start:bt_end]\n",
    "\n",
    "    # 3) Window parameters\n",
    "    window_offset = pd.DateOffset(months=INVESTMENT_WINDOW)\n",
    "\n",
    "    # Step configuration\n",
    "    if step == \"year\":\n",
    "        step_offset = window_offset                   # move forward by 12 months → non-overlapping\n",
    "    elif step == \"day\":\n",
    "        step_offset = pd.DateOffset(days=1)           # move forward by 1 day → overlapping\n",
    "    else:\n",
    "        raise ValueError(\"step must be 'year' or 'day'.\")\n",
    "\n",
    "    # The last window_start must not cause window_end to exceed bt_end\n",
    "    last_start = bt_end - window_offset\n",
    "\n",
    "    all_weights = []\n",
    "\n",
    "    for window_start in pd.date_range(\n",
    "        start=bt_start,\n",
    "        end=last_start,\n",
    "        freq=step_offset,\n",
    "    ):\n",
    "        window_end = window_start + window_offset\n",
    "\n",
    "        feat_slice = full_feat.loc[window_start:window_end]\n",
    "\n",
    "        if feat_slice.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute weights on this slice\n",
    "        # strategy_function should return:\n",
    "        #   - index = dates\n",
    "        #   - values = daily weights (Series)\n",
    "        weight_slice = strategy_function(feat_slice)\n",
    "\n",
    "        all_weights.append(weight_slice)\n",
    "\n",
    "    if not all_weights:\n",
    "        return pd.DataFrame(columns=[\"date\", \"weight\"])\n",
    "\n",
    "    final_series = pd.concat(all_weights, ignore_index=False)\n",
    "\n",
    "    final_series = final_series.reset_index()\n",
    "    final_series.columns = [\"date\", \"weight\"]\n",
    "\n",
    "    return final_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60bbec",
   "metadata": {},
   "source": [
    "## Combine weight and price\n",
    "- Overlapping: step=\"day\"\n",
    "- Non-overlapping: step=\"year\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b9ed95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_weight_price(\n",
    "    btc_df: pd.DataFrame,\n",
    "    backtest_start,\n",
    "    backtest_end,\n",
    "    step\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine extract_weight output with btc_df on date (btc_df index).\n",
    "    Output columns: [\"date\", \"weight\", \"PriceUSD\"]\n",
    "    Keep all overlapping results (dates can appear multiple times)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Get weights (date, weight)\n",
    "    df_weight = extract_weight(\n",
    "        btc_df, \n",
    "        compute_weights,\n",
    "        backtest_start=backtest_start,\n",
    "        backtest_end=backtest_end,\n",
    "        step=step\n",
    "    )\n",
    "\n",
    "    df_weight[\"date\"] = pd.to_datetime(df_weight[\"date\"])\n",
    "\n",
    "    # 2. Prepare btc_df (date is in index)\n",
    "    df_price = btc_df.copy()\n",
    "    df_price.index = pd.to_datetime(df_price.index)\n",
    "\n",
    "    df_price = df_price.reset_index().rename(columns={\"time\": \"date\"})\n",
    "    df_price = df_price[[\"date\", \"PriceUSD\"]]\n",
    "\n",
    "    # 3. Merge on date\n",
    "    df_combo = pd.merge(\n",
    "        df_weight,\n",
    "        df_price,\n",
    "        on=\"date\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 5. Set index (allow duplicate dates)\n",
    "    df_combo = df_combo.set_index(\"date\")\n",
    "\n",
    "    return df_combo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61036697",
   "metadata": {},
   "source": [
    "## Add spd and spd percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d833ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spd_percentile(\n",
    "    file_path: str = \"data/weight_price_ma20.csv\",\n",
    "    output_path: str = \"data/weight_price_ma20_spd_pct.csv\"\n",
    "):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # Define window year (June 1 – May 31)\n",
    "    year = df[\"date\"].dt.year\n",
    "    month = df[\"date\"].dt.month\n",
    "\n",
    "    # June–December => current year, January–May => previous year\n",
    "    df[\"window_year\"] = np.where(month >= 6, year, year - 1)\n",
    "    \n",
    "    # spd = 1 / (BTC/USD) * 100,000,000 \n",
    "    df[\"spd\"] = (1 / df[\"PriceUSD\"]) * 100_000_000\n",
    "\n",
    "    # Get min / max price in each window year\n",
    "    grp = df.groupby(\"window_year\")[\"spd\"]\n",
    "    worst_spd = grp.transform(\"min\")\n",
    "    best_spd = grp.transform(\"max\")\n",
    "\n",
    "    # spd_pct = (your SPD - worst SPD) / (best SPD - worst SPD) * 100\n",
    "    df[\"spd_percentile\"] = (df[\"spd\"] - worst_spd) / (best_spd - worst_spd) * 100\n",
    "\n",
    "    # Drop rows where price_percentile is NaN\n",
    "    df = df.dropna(subset=[\"spd_percentile\"])\n",
    "\n",
    "    # Round to integer\n",
    "    df[\"spd_percentile\"] = df[\"spd_percentile\"].round().astype(int)\n",
    "\n",
    "    # Save file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dd250",
   "metadata": {},
   "source": [
    "## Add dynamic and uniform dca portfolio value (Overlapping window)\n",
    "- Portfolio value = price * BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e69c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dynamic_uniform_portfolios(\n",
    "    file_path: str = \"data/weight_price_olp.csv\",\n",
    "    total_budget: float = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each 12-month rolling window (already concatenated in order in the CSV),\n",
    "    compute, for every row:\n",
    "\n",
    "        - dyn_btc: cumulative BTC using Dynamic DCA\n",
    "        - uni_btc: cumulative BTC using Uniform DCA\n",
    "        - dyn_portfolio_value: portfolio value using Dynamic DCA\n",
    "        - uni_portfolio_value: portfolio value using Uniform DCA\n",
    "\n",
    "    Assumptions:\n",
    "        - CSV has at least columns: [\"date\", \"PriceUSD\", \"weight\"]\n",
    "        - Windows are concatenated in order:\n",
    "            window_0 rows, then window_1 rows, then window_2 rows, ...\n",
    "        - A new window starts when date does NOT strictly increase\n",
    "          (date[t] <= date[t-1]).\n",
    "        - Each window uses the SAME total_budget (e.g., 1.0), and\n",
    "          we fully re-start portfolio in every window.\n",
    "\n",
    "    The function will:\n",
    "        - Keep the same number of rows as the input file.\n",
    "        - Add columns:\n",
    "            \"dyn_btc\", \"uni_btc\",\n",
    "            \"dyn_portfolio_value\", \"uni_portfolio_value\",\n",
    "            \"window_id\", \"start_date\", \"end_date\"\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # Do NOT sort by date globally; we rely on original order to preserve windows.\n",
    "    # Detect new window when date stops strictly increasing.\n",
    "    is_new_window = df[\"date\"].diff().le(pd.Timedelta(0))\n",
    "    is_new_window.iloc[0] = True  # first row is the start of window 1\n",
    "    df[\"window_id\"] = is_new_window.cumsum()\n",
    "\n",
    "    # Prepare new columns\n",
    "    df[\"dyn_portfolio_value\"] = np.nan\n",
    "    df[\"uni_portfolio_value\"] = np.nan\n",
    "    df[\"dyn_portfolio_value\"] = np.nan\n",
    "    df[\"uni_portfolio_value\"] = np.nan\n",
    "    df[\"start_date\"] = pd.NaT\n",
    "    df[\"end_date\"] = pd.NaT\n",
    "\n",
    "    # Process each window independently\n",
    "    for wid, g in df.groupby(\"window_id\", sort=False):\n",
    "        idx = g.index\n",
    "        dates = g[\"date\"].to_numpy()\n",
    "        prices = g[\"PriceUSD\"].astype(float).to_numpy()\n",
    "        weights_raw = g[\"weight\"].astype(float).to_numpy()\n",
    "\n",
    "        # Basic validation\n",
    "        valid_mask = np.isfinite(prices) & np.isfinite(weights_raw) & (prices > 0)\n",
    "        if len(prices) == 0 or not valid_mask.all():\n",
    "            # Leave NaN for this window if invalid data\n",
    "            continue\n",
    "\n",
    "        w_sum = weights_raw.sum()\n",
    "        if w_sum <= 0:\n",
    "            # Cannot normalize weights, skip this window\n",
    "            continue\n",
    "\n",
    "        # Normalize dynamic weights so they sum to 1 within the window\n",
    "        w_dyn = weights_raw / w_sum\n",
    "\n",
    "        n = len(prices)\n",
    "        w_uni = np.full(n, 1.0 / n, dtype=float)\n",
    "\n",
    "        # Allocate the same total_budget in each window\n",
    "        invest_dyn = total_budget * w_dyn\n",
    "        invest_uni = total_budget * w_uni\n",
    "\n",
    "        # Cumulative BTC holdings\n",
    "        btc_dyn = np.cumsum(invest_dyn / prices)\n",
    "        btc_uni = np.cumsum(invest_uni / prices)\n",
    "\n",
    "        # Portfolio value each day\n",
    "        port_dyn = btc_dyn * prices\n",
    "        port_uni = btc_uni * prices\n",
    "\n",
    "        # Write back to df\n",
    "        df.loc[idx, \"dyn_btc\"] = btc_dyn\n",
    "        df.loc[idx, \"uni_btc\"] = btc_uni\n",
    "        df.loc[idx, \"dyn_portfolio_value\"] = port_dyn\n",
    "        df.loc[idx, \"uni_portfolio_value\"] = port_uni\n",
    "        df.loc[idx, \"start_date\"] = dates[0]\n",
    "        df.loc[idx, \"end_date\"] = dates[-1]\n",
    "\n",
    "    # Optional: make window_id an integer type (nullable)\n",
    "    df[\"window_id\"] = df[\"window_id\"].astype(\"Int64\")\n",
    "\n",
    "        # set date as index\n",
    "    df = df.set_index(\"date\")\n",
    "\n",
    "    # reorder columns\n",
    "    df = df[\n",
    "        [\n",
    "            \"weight\",\n",
    "            \"PriceUSD\",\n",
    "            \"window_id\",\n",
    "            \"start_date\",\n",
    "            \"end_date\",\n",
    "            \"dyn_btc\",\n",
    "            \"uni_btc\",\n",
    "            \"dyn_portfolio_value\",\n",
    "            \"uni_portfolio_value\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3efc2",
   "metadata": {},
   "source": [
    "## Add spd and spd percentile (Overlapping window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd88ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spd_and_percentile(\n",
    "    file_path: str = \"data/port_dca.csv\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    window_col: str = \"window_id\",\n",
    "    out_spd_col: str = \"spd\",\n",
    "    out_pct_col: str = \"spd_pct\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Load\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 1) Compute SPD  = (1 / (BTC/USD)) * 100_000_000\n",
    "    df[out_spd_col] = (1 / df[price_col]) * 100_000_000\n",
    "\n",
    "    # 2) SPD Percentile per window\n",
    "    def _calc_pct(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        s = group[out_spd_col].astype(float)\n",
    "        worst = s.min()\n",
    "        best  = s.max()\n",
    "\n",
    "        # avoid division by zero\n",
    "        if np.isclose(best, worst):\n",
    "            group[out_pct_col] = 100.0\n",
    "        else:\n",
    "            group[out_pct_col] = (s - worst) / (best - worst) * 100.0\n",
    "\n",
    "        return group\n",
    "\n",
    "    df = df.groupby(window_col, group_keys=False).apply(_calc_pct)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f5a70",
   "metadata": {},
   "source": [
    "## Merge mstr signal and BTC price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e4b4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_mstr_price(\n",
    "    price_path: str = \"data/btc_data.csv\",\n",
    "    mstr_path: str = \"data/mstr_buy_signal.csv\",\n",
    "    price_time_col: str = \"time\",      # column name in btc_data\n",
    "    mstr_date_col: str = \"date\",       # column name in mstr file\n",
    "    price_col: str = \"PriceUSD\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a clean merged dataset with:\n",
    "        - date (from both sources)\n",
    "        - PriceUSD (BTC price)\n",
    "        - mstr_btc (BTC amount bought by MSTR)\n",
    "        - mstr_usd (USD amount spent by MSTR)\n",
    "\n",
    "    Processing steps:\n",
    "        1. Load BTC price data, rename `time` → `date`, keep only date + PriceUSD.\n",
    "        2. Load MSTR data, ensure date column.\n",
    "        3. Merge on date using left join (price as base).\n",
    "        4. Fill NaN in btc_amount / usd_amount with 0 (days MSTR did not buy).\n",
    "        5. Keep only rows from 2021-01-01 onward.\n",
    "        6. Drop rows where PriceUSD is NaN.\n",
    "        7. Select and rename final columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load and process BTC price data ---\n",
    "    df_price = pd.read_csv(price_path)\n",
    "    df_price[price_time_col] = pd.to_datetime(df_price[price_time_col])\n",
    "    df_price = df_price.rename(columns={price_time_col: \"date\"})\n",
    "    df_price = df_price[[\"date\", price_col]]\n",
    "\n",
    "    # --- Load and process MSTR buy-signal data ---\n",
    "    df_mstr = pd.read_csv(mstr_path)\n",
    "    df_mstr[mstr_date_col] = pd.to_datetime(df_mstr[mstr_date_col])\n",
    "\n",
    "    # Rename to ensure both have column name \"date\"\n",
    "    if mstr_date_col != \"date\":\n",
    "        df_mstr = df_mstr.rename(columns={mstr_date_col: \"date\"})\n",
    "\n",
    "    # --- Merge on date ---\n",
    "    df = pd.merge(df_price, df_mstr, on=\"date\", how=\"left\")\n",
    "\n",
    "    # Fill NaN only for the MSTR buy columns\n",
    "    for col in [\"btc_amount\", \"usd_amount\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Expected column '{col}' not found after merge.\")\n",
    "\n",
    "    # --- Keep only data from 2021-01-01 onward ---\n",
    "    cutoff = pd.Timestamp(\"2021-01-01\")\n",
    "    df = df[df[\"date\"] >= cutoff]\n",
    "\n",
    "    # --- Drop rows with missing BTC price ---\n",
    "    df = df.dropna(subset=[price_col])\n",
    "\n",
    "    # --- Final selection and renaming ---\n",
    "    df = df[[\"date\", price_col, \"btc_amount\", \"usd_amount\"]].rename(\n",
    "        columns={\n",
    "            \"btc_amount\": \"mstr_btc\",\n",
    "            \"usd_amount\": \"mstr_usd\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12153ae",
   "metadata": {},
   "source": [
    "# Run main workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e7205",
   "metadata": {},
   "source": [
    "## Load BTC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b0fe31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df = load_data()\n",
    "validate_price_data(btc_df)\n",
    "btc_df = btc_df.loc[BACKTEST_START:BACKTEST_END]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57cc5a",
   "metadata": {},
   "source": [
    "## Load weight and price through non-overlapping windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97bf8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_p = combine_weight_price(\n",
    "    btc_df,\n",
    "    BACKTEST_START,\n",
    "    BACKTEST_END,\n",
    "    \"year\"\n",
    ")\n",
    "df_w_p.to_csv(\"data/weight_price.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618e491",
   "metadata": {},
   "source": [
    "## Load MA20 through each windows (non-overlapping windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9315007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_p[\"MA200\"] = df_w_p[\"PriceUSD\"].rolling(window=200).mean()\n",
    "df_w_p.to_csv(\"data/weight_price_ma200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc447a5",
   "metadata": {},
   "source": [
    "## Load percentile through each windows (non-overlapping windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "296e34aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/_j6dcxqs3zn8h1179k47j1s80000gn/T/ipykernel_42714/1475057675.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"spd_percentile\"] = df[\"spd_percentile\"].round().astype(int)\n"
     ]
    }
   ],
   "source": [
    "df_w_p_spd_pct = compute_spd_percentile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c54104",
   "metadata": {},
   "source": [
    "## Load price and weight through overlapping window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42d05ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_p_olp = combine_weight_price(\n",
    "    btc_df,\n",
    "    \"2018-06-01\",\n",
    "    \"2025-06-01\",\n",
    "    \"day\"\n",
    ")\n",
    "df_w_p_olp.to_csv(\"data/weight_price_olp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe32a2",
   "metadata": {},
   "source": [
    "## Load Dynamic DCA and Uniform DCA portfolio value (overlapping windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c56ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portfolio = build_dynamic_uniform_portfolios()\n",
    "df_portfolio.to_csv(\"data/port_dca.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e689b3",
   "metadata": {},
   "source": [
    "## Load SPD Percentile across ocerlapping 1-year rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf92e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/_j6dcxqs3zn8h1179k47j1s80000gn/T/ipykernel_61843/1412025504.py:29: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(window_col, group_keys=False).apply(_calc_pct)\n"
     ]
    }
   ],
   "source": [
    "df_spd_pct = add_spd_and_percentile()\n",
    "df_spd_pct\n",
    "df_spd_pct.to_csv(\"data/port_dca_spd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6c53c",
   "metadata": {},
   "source": [
    "## Load Rolling-window SPD backtested Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e26c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spd = compute_cycle_spd(btc_df, compute_weights)\n",
    "df_spd.to_csv(\"data/backtested_btc_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_sats_per_dollar</th>\n",
       "      <th>max_sats_per_dollar</th>\n",
       "      <th>uniform_sats_per_dollar</th>\n",
       "      <th>dynamic_sats_per_dollar</th>\n",
       "      <th>uniform_percentile</th>\n",
       "      <th>dynamic_percentile</th>\n",
       "      <th>excess_percentile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-05-28 → 2025-05-28</th>\n",
       "      <td>897.045908</td>\n",
       "      <td>1857.39123</td>\n",
       "      <td>1300.336244</td>\n",
       "      <td>1759.101721</td>\n",
       "      <td>41.994304</td>\n",
       "      <td>89.765191</td>\n",
       "      <td>47.770887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-29 → 2025-05-29</th>\n",
       "      <td>897.045908</td>\n",
       "      <td>1857.39123</td>\n",
       "      <td>1298.922494</td>\n",
       "      <td>1758.942928</td>\n",
       "      <td>41.847091</td>\n",
       "      <td>89.748656</td>\n",
       "      <td>47.901565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-30 → 2025-05-30</th>\n",
       "      <td>897.045908</td>\n",
       "      <td>1857.39123</td>\n",
       "      <td>1297.507403</td>\n",
       "      <td>1758.803621</td>\n",
       "      <td>41.699739</td>\n",
       "      <td>89.734150</td>\n",
       "      <td>48.034411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-31 → 2025-05-31</th>\n",
       "      <td>897.045908</td>\n",
       "      <td>1857.39123</td>\n",
       "      <td>1296.120833</td>\n",
       "      <td>1758.766249</td>\n",
       "      <td>41.555357</td>\n",
       "      <td>89.730259</td>\n",
       "      <td>48.174902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-01 → 2025-06-01</th>\n",
       "      <td>897.045908</td>\n",
       "      <td>1857.39123</td>\n",
       "      <td>1294.649401</td>\n",
       "      <td>1758.600634</td>\n",
       "      <td>41.402138</td>\n",
       "      <td>89.713013</td>\n",
       "      <td>48.310875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         min_sats_per_dollar  max_sats_per_dollar  \\\n",
       "window                                                              \n",
       "2024-05-28 → 2025-05-28           897.045908           1857.39123   \n",
       "2024-05-29 → 2025-05-29           897.045908           1857.39123   \n",
       "2024-05-30 → 2025-05-30           897.045908           1857.39123   \n",
       "2024-05-31 → 2025-05-31           897.045908           1857.39123   \n",
       "2024-06-01 → 2025-06-01           897.045908           1857.39123   \n",
       "\n",
       "                         uniform_sats_per_dollar  dynamic_sats_per_dollar  \\\n",
       "window                                                                      \n",
       "2024-05-28 → 2025-05-28              1300.336244              1759.101721   \n",
       "2024-05-29 → 2025-05-29              1298.922494              1758.942928   \n",
       "2024-05-30 → 2025-05-30              1297.507403              1758.803621   \n",
       "2024-05-31 → 2025-05-31              1296.120833              1758.766249   \n",
       "2024-06-01 → 2025-06-01              1294.649401              1758.600634   \n",
       "\n",
       "                         uniform_percentile  dynamic_percentile  \\\n",
       "window                                                            \n",
       "2024-05-28 → 2025-05-28           41.994304           89.765191   \n",
       "2024-05-29 → 2025-05-29           41.847091           89.748656   \n",
       "2024-05-30 → 2025-05-30           41.699739           89.734150   \n",
       "2024-05-31 → 2025-05-31           41.555357           89.730259   \n",
       "2024-06-01 → 2025-06-01           41.402138           89.713013   \n",
       "\n",
       "                         excess_percentile  \n",
       "window                                      \n",
       "2024-05-28 → 2025-05-28          47.770887  \n",
       "2024-05-29 → 2025-05-29          47.901565  \n",
       "2024-05-30 → 2025-05-30          48.034411  \n",
       "2024-05-31 → 2025-05-31          48.174902  \n",
       "2024-06-01 → 2025-06-01          48.310875  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spd.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a56335",
   "metadata": {},
   "source": [
    "## Load Allocation Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ff3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 21:16:35 INFO     Animation.save using <class 'matplotlib.animation.PillowWriter'>\n"
     ]
    }
   ],
   "source": [
    "def zscore(s: pd.Series, win: int) -> pd.Series:\n",
    "    m  = s.rolling(win, win // 2).mean()\n",
    "    sd = s.rolling(win, win // 2).std()\n",
    "    return ((s - m) / sd).fillna(0)\n",
    "\n",
    "\n",
    "def fetch_btc() -> pd.Series:\n",
    "    url, cache = (\"https://raw.githubusercontent.com/coinmetrics/data/\"\n",
    "                  \"master/csv/btc.csv\", Path(\"btc.csv\"))\n",
    "    try:\n",
    "        txt = requests.get(url, timeout=15).text\n",
    "        cache.write_text(txt)\n",
    "    except Exception:\n",
    "        txt = cache.read_text()\n",
    "    df = pd.read_csv(io.StringIO(txt), usecols=[\"time\", \"PriceUSD\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.normalize()\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "    return df[\"PriceUSD\"].loc[\"2010-07-18\":].astype(float)\n",
    "\n",
    "price_full = fetch_btc()\n",
    "log_full   = np.log(price_full)\n",
    "price      = price_full.loc[BACKTEST_START:BACKTEST_END]\n",
    "\n",
    "z_all = pd.DataFrame({f\"z{w}\": zscore(log_full, w).clip(-4, 4) for w in WINS})\n",
    "z_lag = z_all.shift(1).fillna(0)\n",
    "\n",
    "FEATS = z_lag.columns.tolist()\n",
    "\n",
    "BACKTEST_START = pd.Timestamp(\"2011-06-01\")\n",
    "BACKTEST_END   = pd.Timestamp(\"2025-06-01\")\n",
    "\n",
    "WINDOW_STARTS = pd.date_range(\n",
    "    BACKTEST_START,\n",
    "    BACKTEST_END - pd.Timedelta(364, \"D\"),\n",
    "    freq=\"D\"\n",
    ")\n",
    "WINDOW_N = len(WINDOW_STARTS)\n",
    "\n",
    "def uniform_spd_pct(idx: pd.DatetimeIndex) -> float:\n",
    "    w  = np.full(len(idx), 1 / len(idx))\n",
    "    btc = (w / price.loc[idx].values).sum()\n",
    "    worst, best = 1 / price.loc[idx].max(), 1 / price.loc[idx].min()\n",
    "    return 100 * (btc - worst) / (best - worst)\n",
    "\n",
    "UNIFORM_PCT = np.array([\n",
    "    uniform_spd_pct(price.loc[s : s + pd.Timedelta(364, \"D\")].index)\n",
    "    for s in WINDOW_STARTS\n",
    "])\n",
    "\n",
    "MONTHLY_MASK = np.isin(\n",
    "    WINDOW_STARTS,\n",
    "    pd.date_range(BACKTEST_START,\n",
    "                  BACKTEST_END - pd.Timedelta(364, \"D\"),\n",
    "                  freq=\"MS\")\n",
    ") \n",
    "\n",
    "# ── parameters & window list ────────────────────────────────────────\n",
    "α = THETA[:18].reshape(3, 6)\n",
    "β = THETA[18:]\n",
    "\n",
    "# keep only one window out of every three monthly starts  →  ~quarterly\n",
    "WINDOW_QUARTERLY = WINDOW_STARTS[MONTHLY_MASK][::3]\n",
    "\n",
    "def beta_curve(n=365, mix=(1/3, 1/3, 1/3)):\n",
    "    x = np.linspace(0.5/n, 1 - 0.5/n, n)\n",
    "    y = (mix[0] * beta.pdf(x, *PROTOS[0]) +\n",
    "         mix[1] * beta.pdf(x, *PROTOS[1]) +\n",
    "         mix[2] * beta.pdf(x, *PROTOS[2]))\n",
    "    return y / y.sum()\n",
    "\n",
    "# ── figure skeleton ────────────────────────────────────────────────\n",
    "fig, (ax_p, ax_w) = plt.subplots(\n",
    "    2, 1, figsize=(8, 5), dpi=150,\n",
    "    gridspec_kw={'height_ratios': [1, 1]},\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "# ── per-frame draw routine ─────────────────────────────────────────\n",
    "def update_frame(k):\n",
    "    start = WINDOW_QUARTERLY[k]\n",
    "    end   = start + pd.Timedelta(364, 'D')\n",
    "    idx   = price_full.loc[start:end].index\n",
    "    if len(idx) < 365:\n",
    "        return\n",
    "\n",
    "    # ----- weights -------------------------------------------------------\n",
    "    z0   = z_lag.loc[start, FEATS].values\n",
    "    mix  = softmax(α @ np.r_[1, z0])\n",
    "    base = beta_curve(365, mix)\n",
    "    mod  = np.exp(-(z_lag.loc[idx, FEATS].values @ β))\n",
    "    w    = allocate_sequential(base * mod)\n",
    "\n",
    "    # ----- clear axes ----------------------------------------------------\n",
    "    ax_p.cla()\n",
    "    ax_w.cla()\n",
    "\n",
    "    # ----- price panel ---------------------------------------------------\n",
    "    ax_p.plot(idx, price_full.loc[idx], lw=1.4, color='black')\n",
    "    ax_p.set_ylabel(\"BTC price [USD]\", fontsize=9)\n",
    "    ax_p.set_xlim(idx.min(), idx.max())\n",
    "    ax_p.grid(alpha=0.25)\n",
    "\n",
    "    # ----- allocation panel (log-scale) ---------------------------------\n",
    "    xnum = mdates.date2num(idx)\n",
    "    ax_w.plot(idx, base, lw=1.2, color='lightgray', alpha=0.5)\n",
    "    ax_w.vlines(xnum, base, w, color='orange', lw=2.2, alpha=0.5)\n",
    "    ax_w.scatter(idx, w, color='orange', s=20, zorder=3, alpha=1)\n",
    "    ax_w.set_yscale('log')\n",
    "    ax_w.set_ylabel(\"Weight (log)\", fontsize=9)\n",
    "    ax_w.set_xlim(idx.min(), idx.max())\n",
    "    ax_w.set_ylim(MIN_W * 0.8, w.max() * 1.4)\n",
    "    ax_w.grid(alpha=0.25)\n",
    "\n",
    "    # ----- shared X-axis formatting -------------------------------------\n",
    "    locator   = mdates.AutoDateLocator()\n",
    "    formatter = mdates.DateFormatter(\"%b %Y\")\n",
    "    for ax in (ax_p, ax_w):\n",
    "        ax.xaxis.set_major_locator(locator)\n",
    "        ax.xaxis.set_major_formatter(formatter)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "        ax.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "# ── build & save GIF (1 fps) ────────────────────────────────────────\n",
    "anim = FuncAnimation(\n",
    "    fig,\n",
    "    update_frame,\n",
    "    frames=len(WINDOW_QUARTERLY),\n",
    "    interval=1200,    # 500 ms per frame\n",
    "    blit=False\n",
    ")\n",
    "\n",
    "anim.save(\"data/allocation_animation_quarterly.gif\", writer=PillowWriter(fps=1))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4e574",
   "metadata": {},
   "source": [
    "## Load MSTR buying amount and BTC price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17b2e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mstr_price = merge_mstr_price()\n",
    "df_mstr_price.to_csv(\"data/mstr_price.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d33bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
