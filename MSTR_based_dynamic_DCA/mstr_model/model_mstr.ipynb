{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4005bb8",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0954f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from io import StringIO\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e1ccc",
   "metadata": {},
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0419c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back-test date range\n",
    "BACKTEST_START     = '2021-01-01' \n",
    "BACKTEST_END       = '2025-11-07' \n",
    "# BACKTEST_START     = '2011-06-01' \n",
    "# BACKTEST_END       = '2025-06-01' \n",
    "\n",
    "# Rolling window length (in months)\n",
    "INVESTMENT_WINDOW  = 12\n",
    "\n",
    "# Step frequency for window start-dates: 'Daily', 'Weekly' or 'Monthly'\n",
    "PURCHASE_FREQ      = 'Daily'\n",
    "\n",
    "# Minimum per-period weight (to avoid zero allocations)\n",
    "MIN_WEIGHT         = 1e-5\n",
    "\n",
    "PURCHASE_FREQ_TO_OFFSET = {\n",
    "    'Daily':   '1D',\n",
    "    'Weekly':  '7D',\n",
    "    'Monthly': '1M',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1e616f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4ed3b",
   "metadata": {},
   "source": [
    "# Download BTC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d196e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from coinmetrics.api_client import CoinMetricsClient\n",
    "except ImportError:\n",
    "    raise ImportError(\"coinmetrics.api_client module is required. Install it via pip:\\n\\n    pip install coinmetrics-api-client\")\n",
    "\n",
    "def extract_btc_data_to_csv(local_path='btc_data.csv'):\n",
    "    # Coin Metrics BTC CSV (raw GitHub URL)\n",
    "    url = \"https://raw.githubusercontent.com/coinmetrics/data/master/csv/btc.csv\"\n",
    "    \n",
    "    # Download the content\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # raises an error for bad responses\n",
    "    \n",
    "    # Parse CSV content\n",
    "    btc_df = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "    btc_df['time'] = pd.to_datetime(btc_df['time']).dt.normalize()\n",
    "    btc_df['time'] = btc_df['time'].dt.tz_localize(None)\n",
    "    btc_df.set_index('time', inplace=True)\n",
    "\n",
    "    btc_df.to_csv(local_path)\n",
    "    \n",
    "    # Show the df\n",
    "    btc_df\n",
    "\n",
    "btc_df = extract_btc_data_to_csv(\"btc_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa96c44",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "954a2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(\"btc_data.csv\", index_col=0, parse_dates=True)\n",
    "    df = df.loc[~df.index.duplicated(keep='last')]\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "def validate_price_data(df):\n",
    "    if df.empty or 'PriceUSD' not in df.columns:\n",
    "        raise ValueError(\"Invalid BTC price data.\")\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Index must be datetime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc2da1",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761bf04",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be239c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FULL_FEATURES = None\n",
    "\n",
    "MIN_W = 1e-5\n",
    "WINS = [30, 90, 180, 365, 1461]\n",
    "FEATS = [f\"z{w}\" for w in WINS]\n",
    "PROTOS = [(0.5, 5.0), (1.0, 1.0), (5.0, 0.5)]\n",
    "\n",
    "# Optimized theta parameters from the final model run\n",
    "THETA = np.array([\n",
    "    1.3507, 1.073, -1.226, 2.5141, 2.9946, -0.4083, -0.1082, -0.6809,\n",
    "    0.3465, -0.6804, -2.9974, -2.9991, -1.2658, -0.368, 0.7567, -1.9627,\n",
    "    -1.9124, 2.9983, 0.5704, 0.0, 0.8669, 1.2546, 5.0\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574063f",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba738b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts a vector of scores into a probability distribution.\"\"\"\n",
    "    ex = np.exp(x - x.max())\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def allocate_sequential(raw: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Strict left-to-right 'drain' allocator.\"\"\"\n",
    "    n = len(raw)\n",
    "    floor = n * MIN_W\n",
    "    rem_budget, rem_raw = 1 - floor, raw.sum()\n",
    "    w = np.empty_like(raw)\n",
    "    for i, x in enumerate(raw):\n",
    "        share = 0 if rem_raw == 0 else (x / rem_raw) * rem_budget\n",
    "        w[i] = MIN_W + share\n",
    "        rem_budget -= share\n",
    "        rem_raw -= x\n",
    "    return w / w.sum()\n",
    "\n",
    "def beta_mix_pdf(n: int, mix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Generates a smooth baseline curve from a mixture of Beta distributions.\"\"\"\n",
    "    t = np.linspace(0.5 / n, 1 - 0.5 / n, n)\n",
    "    return (mix[0] * beta.pdf(t, *PROTOS[0]) +\n",
    "            mix[1] * beta.pdf(t, *PROTOS[1]) +\n",
    "            mix[2] * beta.pdf(t, *PROTOS[2])) / n\n",
    "\n",
    "def zscore(s: pd.Series, win: int) -> pd.Series:\n",
    "    \"\"\"Calculates the rolling z-score for a given series and window.\"\"\"\n",
    "    m = s.rolling(win, win // 2).mean()\n",
    "    sd = s.rolling(win, win // 2).std()\n",
    "    return ((s - m) / sd).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5d7fd",
   "metadata": {},
   "source": [
    "## Main Strategy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dea94e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates features on the full historical data ONCE and returns the\n",
    "    relevant slice. This robustly handles calls from different boilerplate\n",
    "    functions and avoids boundary errors that cause leakage.\n",
    "    \"\"\"\n",
    "    global _FULL_FEATURES\n",
    "\n",
    "    # Only compute the full feature set if it hasn't been done yet.\n",
    "    if _FULL_FEATURES is None:\n",
    "        try:\n",
    "            # Assumes 'btc_data.csv' is in the same directory.\n",
    "            full_price_df = pd.read_csv(\"btc_data.csv\", index_col=0, parse_dates=True)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"btc_data.csv not found. Please ensure it's in the correct directory.\")\n",
    "        \n",
    "        # Select only the PriceUSD column before doing anything else.\n",
    "        full_price_df = full_price_df[['PriceUSD']]\n",
    "        \n",
    "        # We need history from before the backtest start date for rolling windows.\n",
    "        full_price_df = full_price_df.loc[\"2020-02-18\":]\n",
    "        # full_price_df = full_price_df.loc[\"2010-07-18\":] \n",
    "\n",
    "        log_prices = np.log(full_price_df['PriceUSD'])\n",
    "        \n",
    "        z_all = pd.DataFrame({f\"z{w}\": zscore(log_prices, w).clip(-4, 4) for w in WINS}, index=log_prices.index)\n",
    "        \n",
    "        # The strategy uses lagged features to avoid look-ahead bias.\n",
    "        z_lag = z_all.shift(1).fillna(0)\n",
    "        \n",
    "        _FULL_FEATURES = full_price_df.join(z_lag)\n",
    "\n",
    "    # Return the portion of the pre-computed features that matches the input index.\n",
    "    return _FULL_FEATURES.reindex(df.index).fillna(0)\n",
    "\n",
    "\n",
    "def compute_weights(df_window: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given a slice of data, computes portfolio weights that sum to 1.\n",
    "    Minimal change: use MSTR buy dates as the only signal.\n",
    "      - If the window has ≥1 signal day: everyone gets MIN_WEIGHT,\n",
    "        leftover is split equally across signal days.\n",
    "      - If 0 signal days: daily uniform.\n",
    "    Always returns weights >= MIN_WEIGHT and sum = 1.\n",
    "    \"\"\"\n",
    "    if df_window.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    # Keep feature pipeline untouched\n",
    "    feat_slice = construct_features(df_window)\n",
    "\n",
    "    idx = pd.DatetimeIndex(feat_slice.index).normalize()\n",
    "    n = len(idx)\n",
    "\n",
    "    mstr_dates = set(\n",
    "        pd.to_datetime(\n",
    "            pd.read_csv(\"mstr_buy_signal.csv\")[\"purchase_date\"],\n",
    "            errors=\"coerce\"\n",
    "        ).dropna().dt.normalize()\n",
    "    )\n",
    "\n",
    "    # ---- signal days in this window ----\n",
    "    signal_mask = idx.isin(mstr_dates)\n",
    "    signal_days = np.where(signal_mask)[0]\n",
    "    n_signal = len(signal_days)\n",
    "\n",
    "    # ---- no signal → uniform ----\n",
    "    if n_signal == 0:\n",
    "        w = np.full(n, 1.0 / n, dtype=float)\n",
    "        w = np.clip(w, MIN_WEIGHT, None)\n",
    "        w = w / w.sum()\n",
    "        return pd.Series(w, index=feat_slice.index)\n",
    "\n",
    "    # ---- has signal → MIN_WEIGHT + leftover equally on signals ----\n",
    "    w = np.full(n, MIN_WEIGHT, dtype=float)\n",
    "    base = n * MIN_WEIGHT\n",
    "    leftover = max(0.0, 1.0 - base)\n",
    "    add_per_signal = leftover / n_signal if leftover > 0 else 0.0\n",
    "    w[signal_days] += add_per_signal\n",
    "\n",
    "    # numerical safety\n",
    "    w = np.clip(w, MIN_WEIGHT, None)\n",
    "\n",
    "    return pd.Series(w, index=feat_slice.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83f170",
   "metadata": {},
   "source": [
    "# Run Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3096e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_window_label(window_start: pd.Timestamp, window_end: pd.Timestamp) -> str:\n",
    "    \"\"\"\n",
    "    Format \"YYYY-MM-DD → YYYY-MM-DD\" for a rolling window.\n",
    "    \"\"\"\n",
    "    start_str = pd.to_datetime(window_start).strftime(\"%Y-%m-%d\")\n",
    "    end_str   = pd.to_datetime(window_end).strftime(\"%Y-%m-%d\")\n",
    "    return f\"{start_str} → {end_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c7e18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cycle_spd(\n",
    "    dataframe: pd.DataFrame,\n",
    "    strategy_function\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute sats‐per‐dollar (SPD) stats over rolling windows.\n",
    "\n",
    "    - Uses full‐history features (no look‐ahead).\n",
    "    - Window length = INVESTMENT_WINDOW months.\n",
    "    - Step every PURCHASE_FREQ.\n",
    "    - Returns a DataFrame indexed by window label, with:\n",
    "        min_sats_per_dollar, max_sats_per_dollar,\n",
    "        uniform_sats_per_dollar, dynamic_sats_per_dollar,\n",
    "        uniform_percentile, dynamic_percentile, excess_percentile.\n",
    "    \"\"\"\n",
    "    # 1) Precompute full-history features & restrict to backtest\n",
    "    full_feat = construct_features(dataframe).loc[BACKTEST_START:BACKTEST_END]\n",
    "\n",
    "    # 2) Window parameters\n",
    "    window_offset  = pd.DateOffset(months=INVESTMENT_WINDOW)\n",
    "    step_freq      = PURCHASE_FREQ_TO_OFFSET[PURCHASE_FREQ]\n",
    "\n",
    "    results = []\n",
    "    for window_start in pd.date_range(\n",
    "        start=pd.to_datetime(BACKTEST_START),\n",
    "        end=pd.to_datetime(BACKTEST_END) - window_offset,\n",
    "        freq=step_freq\n",
    "    ):\n",
    "        window_end  = window_start + window_offset\n",
    "        feat_slice  = full_feat.loc[window_start:window_end]\n",
    "        price_slice = dataframe[\"PriceUSD\"].loc[window_start:window_end]\n",
    "\n",
    "        if price_slice.empty:\n",
    "            continue\n",
    "\n",
    "        label       = _make_window_label(window_start, window_end)\n",
    "        inv_price   = (1.0 / price_slice) * 1e8  # sats per dollar\n",
    "\n",
    "        # Compute weights on this slice\n",
    "        weight_slice = strategy_function(feat_slice)\n",
    "\n",
    "        # Uniform vs. dynamic SPD\n",
    "        uniform_spd = inv_price.mean()\n",
    "        dynamic_spd = (weight_slice * inv_price).sum()\n",
    "\n",
    "        # Min/max for percentile scaling\n",
    "        min_spd = inv_price.min()   # low price → high SPD\n",
    "        max_spd = inv_price.max()   # high price → low SPD\n",
    "        span    = max_spd - min_spd\n",
    "\n",
    "        uniform_pct = (uniform_spd - min_spd) / span * 100\n",
    "        dynamic_pct = (dynamic_spd - min_spd) / span * 100\n",
    "\n",
    "        results.append({\n",
    "            \"window\":                   label,\n",
    "            \"min_sats_per_dollar\":      min_spd,\n",
    "            \"max_sats_per_dollar\":      max_spd,\n",
    "            \"uniform_sats_per_dollar\":  uniform_spd,\n",
    "            \"dynamic_sats_per_dollar\":  dynamic_spd,\n",
    "            \"uniform_percentile\":       uniform_pct,\n",
    "            \"dynamic_percentile\":       dynamic_pct,\n",
    "            \"excess_percentile\":        dynamic_pct - uniform_pct,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).set_index(\"window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50489854",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b64c7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_dynamic_dca(\n",
    "    dataframe: pd.DataFrame,\n",
    "    strategy_function,\n",
    "    *,\n",
    "    strategy_label: str = \"strategy\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Runs compute_cycle_spd(...)\n",
    "    2) Prints aggregated min/max/mean/median of dynamic SPD\n",
    "    3) Prints aggregated SPD percentiles\n",
    "    4) Computes & prints exponentially-decayed average SPD and percentile\n",
    "    5) Returns the full SPD table.\n",
    "\n",
    "    Exponential decay:\n",
    "      • decay_rate ∈ (0,1): lower → faster decay\n",
    "      • most recent window has highest weight\n",
    "      • weights normalized to sum to 1\n",
    "    \"\"\"\n",
    "    # --- run the rolling-window SPD backtest\n",
    "    spd_table   = compute_cycle_spd(dataframe, strategy_function)\n",
    "    dynamic_spd = spd_table[\"dynamic_sats_per_dollar\"]\n",
    "    dynamic_pct = spd_table[\"dynamic_percentile\"]\n",
    "\n",
    "    # --- print standard aggregated metrics\n",
    "    print(f\"\\nAggregated Metrics for {strategy_label}:\")\n",
    "    print(\"Dynamic Sats-per-Dollar:\")\n",
    "    for stat in (\"min\", \"max\", \"mean\", \"median\"):\n",
    "        val = getattr(dynamic_spd, stat)()\n",
    "        print(f\"  {stat}: {val:.2f}\")\n",
    "\n",
    "    print(\"\\nDynamic SPD Percentiles:\")\n",
    "    for stat in (\"min\", \"max\", \"mean\", \"median\"):\n",
    "        val = getattr(dynamic_pct, stat)()\n",
    "        print(f\"  {stat}: {val:.2f}%\")\n",
    "\n",
    "    # --- exponential decay weighting\n",
    "    decay_rate = 0.9\n",
    "    N = len(dynamic_spd)\n",
    "    # weight for window i (0 = oldest, N-1 = newest)\n",
    "    raw_weights = np.array([decay_rate ** (N - 1 - i) for i in range(N)])\n",
    "    exp_weights = raw_weights / raw_weights.sum()\n",
    "\n",
    "    # --- compute decayed averages\n",
    "    exp_avg_spd = (dynamic_spd.values * exp_weights).sum()\n",
    "    exp_avg_pct = (dynamic_pct.values * exp_weights).sum()\n",
    "\n",
    "    # --- print decayed metrics\n",
    "    print(f\"\\nExponential-Decay Average SPD: {exp_avg_spd:.2f}\")\n",
    "    print(f\"Exponential-Decay Average SPD Percentile: {exp_avg_pct:.2f}%\")\n",
    "\n",
    "    return spd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dfdc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_strategy_submission_ready(\n",
    "    dataframe: pd.DataFrame,\n",
    "    strategy_function\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Sanity-check that `strategy_function`:\n",
    "      1. Uses no future data (forward-leakage test).\n",
    "      2. Produces weights ≥ MIN_WEIGHT.\n",
    "      3. Sums to 1.0 in each rolling window.\n",
    "      4. Outperforms uniform DCA in at least 50% of rolling windows.\n",
    "    \"\"\"\n",
    "    passed = True\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    # 1) Forward-leakage test\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    backtest_df  = dataframe.loc[BACKTEST_START:BACKTEST_END]\n",
    "    full_weights = strategy_function(dataframe) \\\n",
    "                       .reindex(backtest_df.index) \\\n",
    "                       .fillna(0.0)\n",
    "\n",
    "    step_dates = max(len(backtest_df) // 50, 1)\n",
    "    probe_dates = backtest_df.index[::step_dates]\n",
    "\n",
    "    for probe in probe_dates:\n",
    "        masked = dataframe.copy()\n",
    "        masked.loc[masked.index > probe, :] = np.nan\n",
    "\n",
    "        masked_wt = strategy_function(masked) \\\n",
    "                        .reindex(full_weights.index) \\\n",
    "                        .fillna(0.0)\n",
    "\n",
    "        if not np.isclose(masked_wt.loc[probe],\n",
    "                          full_weights.loc[probe],\n",
    "                          rtol=1e-9, atol=1e-12):\n",
    "            delta = abs(masked_wt.loc[probe] - full_weights.loc[probe])\n",
    "            print(f\"[{probe.date()}] ❌ Forward-leakage detected (Δ={delta:.2e})\")\n",
    "            passed = False\n",
    "            break\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    # 2) Weight checks per rolling window\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    window_offset = pd.DateOffset(months=INVESTMENT_WINDOW)\n",
    "    step_freq     = PURCHASE_FREQ_TO_OFFSET[PURCHASE_FREQ]\n",
    "\n",
    "    for window_start in pd.date_range(\n",
    "        start=pd.to_datetime(BACKTEST_START),\n",
    "        end=pd.to_datetime(BACKTEST_END) - window_offset,\n",
    "        freq=step_freq\n",
    "    ):\n",
    "        window_end = window_start + window_offset\n",
    "        label      = _make_window_label(window_start, window_end)\n",
    "\n",
    "        w_slice = strategy_function(dataframe.loc[window_start:window_end])\n",
    "\n",
    "        if (w_slice <= 0).any():\n",
    "            print(f\"[{label}] ❌ Non-positive weights detected.\")\n",
    "            passed = False\n",
    "\n",
    "        if (w_slice < MIN_WEIGHT).any():\n",
    "            print(f\"[{label}] ❌ Weight below MIN_WEIGHT = {MIN_WEIGHT}.\")\n",
    "            passed = False\n",
    "\n",
    "        total = w_slice.sum()\n",
    "        if not np.isclose(total, 1.0, rtol=1e-5, atol=1e-8):\n",
    "            print(f\"[{label}] ❌ Sum-to-1 check failed: {total:.4f}\")\n",
    "            passed = False\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    # 3) Performance vs. Uniform DCA (RELAXED)\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    spd_table = compute_cycle_spd(dataframe, strategy_function)\n",
    "\n",
    "    underperf_records = []\n",
    "    for label, row in spd_table.iterrows():\n",
    "        dp, up = row[\"dynamic_percentile\"], row[\"uniform_percentile\"]\n",
    "        if dp < up:\n",
    "            underperf_records.append({\n",
    "                \"Window\": label,\n",
    "                \"Dynamic Percentile\": dp,\n",
    "                \"Uniform Percentile\": up,\n",
    "                \"Delta\": dp - up\n",
    "            })\n",
    "\n",
    "    total = len(spd_table)\n",
    "    failed = len(underperf_records)\n",
    "    pass_ratio = (total - failed) / total\n",
    "\n",
    "    if underperf_records:\n",
    "        df_underperf = pd.DataFrame(underperf_records)\n",
    "        print(\"\\n⚠️ Windows where strategy underperformed Uniform DCA:\")\n",
    "        display(df_underperf)\n",
    "\n",
    "    print(f\"\\nSummary: Your strategy underperformed uniform DCA in {failed} out of {total} windows \"\n",
    "          f\"({100 * pass_ratio:.2f}% win rate)\")\n",
    "\n",
    "    if pass_ratio >= 0.5:\n",
    "        print(\"✅ Strategy meets performance requirement (≥ 50% win rate vs. uniform DCA).\")\n",
    "    else:\n",
    "        print(\"❌ Strategy failed performance requirement (< 50% win rate vs. uniform DCA).\")\n",
    "        passed = False\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    # Final verdict\n",
    "    # ──────────────────────────────────────────────────────────\n",
    "    if passed:\n",
    "        print(\"\\n✅ Strategy is ready for submission.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Please address the above issues before submitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f38f05",
   "metadata": {},
   "source": [
    "# Run main workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7675039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df = load_data()\n",
    "validate_price_data(btc_df)\n",
    "btc_df = btc_df.loc[BACKTEST_START:BACKTEST_END]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cbdfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Metrics for Dynamic DCA:\n",
      "Dynamic Sats-per-Dollar:\n",
      "  min: 985.96\n",
      "  max: 4710.91\n",
      "  mean: 2627.27\n",
      "  median: 2416.72\n",
      "\n",
      "Dynamic SPD Percentiles:\n",
      "  min: 17.60%\n",
      "  max: 59.63%\n",
      "  mean: 33.99%\n",
      "  median: 32.92%\n",
      "\n",
      "Exponential-Decay Average SPD: 987.99\n",
      "Exponential-Decay Average SPD Percentile: 28.71%\n"
     ]
    }
   ],
   "source": [
    "# Rolling-window SPD backtest:\n",
    "df_spd = backtest_dynamic_dca(\n",
    "    btc_df,\n",
    "    compute_weights,\n",
    "    strategy_label=\"Dynamic DCA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d587074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ Windows where strategy underperformed Uniform DCA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window</th>\n",
       "      <th>Dynamic Percentile</th>\n",
       "      <th>Uniform Percentile</th>\n",
       "      <th>Delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01 → 2022-01-01</td>\n",
       "      <td>34.854574</td>\n",
       "      <td>37.703488</td>\n",
       "      <td>-2.848915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02 → 2022-01-02</td>\n",
       "      <td>35.672560</td>\n",
       "      <td>38.401232</td>\n",
       "      <td>-2.728672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03 → 2022-01-03</td>\n",
       "      <td>35.672044</td>\n",
       "      <td>38.260164</td>\n",
       "      <td>-2.588120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04 → 2022-01-04</td>\n",
       "      <td>35.671604</td>\n",
       "      <td>38.139957</td>\n",
       "      <td>-2.468353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05 → 2022-01-05</td>\n",
       "      <td>35.671153</td>\n",
       "      <td>38.016756</td>\n",
       "      <td>-2.345602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>2024-11-03 → 2025-11-03</td>\n",
       "      <td>27.415506</td>\n",
       "      <td>28.902610</td>\n",
       "      <td>-1.487105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>2024-11-04 → 2025-11-04</td>\n",
       "      <td>27.414811</td>\n",
       "      <td>28.712959</td>\n",
       "      <td>-1.298148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>2024-11-05 → 2025-11-05</td>\n",
       "      <td>29.020276</td>\n",
       "      <td>30.175202</td>\n",
       "      <td>-1.154927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>2024-11-06 → 2025-11-06</td>\n",
       "      <td>35.549011</td>\n",
       "      <td>36.728025</td>\n",
       "      <td>-1.179014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>2024-11-07 → 2025-11-07</td>\n",
       "      <td>35.879825</td>\n",
       "      <td>36.882648</td>\n",
       "      <td>-1.002823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Window  Dynamic Percentile  Uniform Percentile  \\\n",
       "0     2021-01-01 → 2022-01-01           34.854574           37.703488   \n",
       "1     2021-01-02 → 2022-01-02           35.672560           38.401232   \n",
       "2     2021-01-03 → 2022-01-03           35.672044           38.260164   \n",
       "3     2021-01-04 → 2022-01-04           35.671604           38.139957   \n",
       "4     2021-01-05 → 2022-01-05           35.671153           38.016756   \n",
       "...                       ...                 ...                 ...   \n",
       "1152  2024-11-03 → 2025-11-03           27.415506           28.902610   \n",
       "1153  2024-11-04 → 2025-11-04           27.414811           28.712959   \n",
       "1154  2024-11-05 → 2025-11-05           29.020276           30.175202   \n",
       "1155  2024-11-06 → 2025-11-06           35.549011           36.728025   \n",
       "1156  2024-11-07 → 2025-11-07           35.879825           36.882648   \n",
       "\n",
       "         Delta  \n",
       "0    -2.848915  \n",
       "1    -2.728672  \n",
       "2    -2.588120  \n",
       "3    -2.468353  \n",
       "4    -2.345602  \n",
       "...        ...  \n",
       "1152 -1.487105  \n",
       "1153 -1.298148  \n",
       "1154 -1.154927  \n",
       "1155 -1.179014  \n",
       "1156 -1.002823  \n",
       "\n",
       "[1157 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary: Your strategy underperformed uniform DCA in 1157 out of 1407 windows (17.77% win rate)\n",
      "❌ Strategy failed performance requirement (< 50% win rate vs. uniform DCA).\n",
      "\n",
      "⚠️ Please address the above issues before submitting.\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks (each window inside):\n",
    "check_strategy_submission_ready(btc_df, compute_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "519c9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win_rate = 99.79\n",
    "# exp_decay_percentile = 68.77\n",
    "\n",
    "# score = 0.5 * win_rate + 0.5 * exp_decay_percentile\n",
    "# print(f\"Final Model Score (50/50 weighting): {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9810a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
