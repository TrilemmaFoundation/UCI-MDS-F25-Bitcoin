{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e62747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683cb77",
   "metadata": {},
   "source": [
    "# 1.2 Allocation vs Price Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec181a97",
   "metadata": {},
   "source": [
    "## Mean allocation weight by BTC price quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d82a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_quartile_summary(file_path: str = \"data/weight_price.csv\", show_plot: bool = True):    \n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.set_index(\"date\")\n",
    "\n",
    "    # Create price quartiles\n",
    "    df[\"price_quartile\"] = pd.qcut(\n",
    "        df[\"PriceUSD\"],\n",
    "        4,\n",
    "        labels=[\"cheapest\", \"low\", \"high\", \"expensive\"]\n",
    "    )\n",
    "\n",
    "    # Aggregate weight stats by quartile\n",
    "    quart = df.groupby(\"price_quartile\")[\"weight\"].agg(\n",
    "        [\"mean\", \"count\"]\n",
    "    )\n",
    "\n",
    "    # Plot with matplotlib\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.bar(quart.index, quart[\"mean\"])\n",
    "        plt.xlabel(\"Price Quartile\")\n",
    "        plt.ylabel(\"Mean Weight\")\n",
    "        plt.title(\"Mean Allocation Weight by BTC Price Quartile\")\n",
    "        plt.show()\n",
    "\n",
    "    return quart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c60b5",
   "metadata": {},
   "source": [
    "## BTC Price and Dynamic DCA Allocation under Non-overlapping 365-Days Rolling Windows (2018–2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75865fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_price_by_year(\n",
    "    start_year: int,\n",
    "    end_year: int | None = None,\n",
    "    file_path: str = \"data/weight_price.csv\"\n",
    "):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # If only one year is given\n",
    "    if end_year is None:\n",
    "        end_year = start_year + 1\n",
    "\n",
    "    # Window: June 1 -> May 31\n",
    "    start_date = pd.Timestamp(f\"{start_year}-06-01\")\n",
    "    end_date   = pd.Timestamp(f\"{end_year}-05-31\")\n",
    "\n",
    "    # Filter by window\n",
    "    df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
    "\n",
    "    # Plot\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # BTC Price (left axis)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"date\"],\n",
    "            y=df[\"PriceUSD\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"BTC Price\"\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "\n",
    "    # Weight (right axis)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df[\"date\"],\n",
    "            y=df[\"weight\"],\n",
    "            name=\"Weight\",\n",
    "            marker=dict(color=\"red\", opacity=1)\n",
    "        ),\n",
    "        secondary_y=True\n",
    "    )\n",
    "\n",
    "    # X-axis: every June 1st\n",
    "    fig.update_layout(\n",
    "        title=f\"BTC Price and Allocation Weight ({start_year} → {end_year})\",\n",
    "        xaxis=dict(\n",
    "            tickmode=\"linear\",\n",
    "            dtick=\"M12\",\n",
    "            tick0=f\"{start_year}-06-01\"\n",
    "        ),\n",
    "        barmode=\"overlay\"\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(title_text=\"BTC Price (USD)\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Weight\", secondary_y=True)\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12970721",
   "metadata": {},
   "source": [
    "# 1.3 Allocation Behavior by Market Regime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6583bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_allocation_by_regime(\n",
    "    file_path: str = \"data/weight_price_ma20.csv\",\n",
    "    buffer: float = 0.05,\n",
    ") -> pd.DataFrame:\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    price = df[\"PriceUSD\"]\n",
    "    ma = df[\"MA20\"]\n",
    "\n",
    "    # Define regimes based on Price vs MA20 with a buffer\n",
    "    conditions = [\n",
    "        price > ma * (1 + buffer),  # clearly above MA20 → bull\n",
    "        price < ma * (1 - buffer),  # clearly below MA20 → bear\n",
    "    ]\n",
    "    choices = [\"bull\", \"bear\"]\n",
    "\n",
    "    df[\"regime\"] = np.select(conditions, choices, default=\"sideways\")\n",
    "\n",
    "    # Mean allocation weight per regime\n",
    "    regime_alloc = df.groupby(\"regime\", as_index=False)[\"weight\"].mean()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(regime_alloc[\"regime\"], regime_alloc[\"weight\"])\n",
    "    plt.xlabel(\"Market Regime (based on MA20)\")\n",
    "    plt.ylabel(\"Mean Allocation Weight\")\n",
    "    plt.title(\"Mean Daily Allocation by Market Regime\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return regime_alloc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48ce01",
   "metadata": {},
   "source": [
    "# 2.1 SPD Percentile vs Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d220fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentile_vs_weight(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct.csv\",\n",
    "    figsize: tuple = (6, 4),\n",
    "    jitter_std: float = 0.3,\n",
    "    alpha: float = 0.25\n",
    "):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert to numeric and clean\n",
    "    df[\"spd_percentile\"] = pd.to_numeric(df[\"spd_percentile\"], errors=\"coerce\")\n",
    "    df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"spd_percentile\", \"weight\"])\n",
    "\n",
    "    # Add jitter on x-axis\n",
    "    jitter = np.random.normal(0, jitter_std, size=len(df))\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(\n",
    "        df[\"spd_percentile\"] + jitter,\n",
    "        df[\"weight\"],\n",
    "        alpha=alpha\n",
    "    )\n",
    "    plt.xlabel(\"SPD Percentile\")\n",
    "    plt.ylabel(\"Weight\")\n",
    "    plt.title(\"SPD Percentile vs Weight (2011–2025)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6754bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_by_spd_bucket(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct.csv\",\n",
    "    bucket_size: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a boxplot of weight distribution by SPD percentile buckets,\n",
    "    and highlight the mean value with a red dot (log scale).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert columns to numeric and remove NaN rows\n",
    "    df[\"spd_percentile\"] = pd.to_numeric(df[\"spd_percentile\"], errors=\"coerce\")\n",
    "    df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"spd_percentile\", \"weight\"])\n",
    "\n",
    "    # Create percentile buckets (e.g. 0–20, 20–40, ...)\n",
    "    bins = list(range(0, 100 + bucket_size, bucket_size))\n",
    "    df[\"bucket\"] = pd.cut(df[\"spd_percentile\"], bins=bins)\n",
    "\n",
    "    # Collect values for each bucket\n",
    "    bucket_groups = []\n",
    "    labels = []\n",
    "\n",
    "    for name, group in df.groupby(\"bucket\", observed=True):\n",
    "        bucket_groups.append(group[\"weight\"].values)\n",
    "        labels.append(str(name))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    bp = plt.boxplot(\n",
    "        bucket_groups,\n",
    "        labels=labels,\n",
    "        showfliers=True,\n",
    "        showmeans=True,\n",
    "        meanline=False\n",
    "    )\n",
    "\n",
    "    # Use log scale for better visibility\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    # Force the mean points to be red and more visible\n",
    "    for m in bp[\"means\"]:\n",
    "        m.set_marker(\"o\")\n",
    "        m.set_markerfacecolor(\"red\")\n",
    "        m.set_markeredgecolor(\"red\")\n",
    "        m.set_markersize(7)\n",
    "\n",
    "    # Add custom legend for the mean\n",
    "    plt.scatter([], [], color=\"red\", label=\"Mean Weight\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"SPD Percentile Bucket\")\n",
    "    plt.ylabel(\"Weight (log scale)\")\n",
    "    plt.title(\"Weight Distribution by SPD Percentile Bucket (Mean Highlighted)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ad905",
   "metadata": {},
   "source": [
    "# 2.2 Relationship Between SPD Percentile & Future Returns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e673a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weighted_forward_returns_with_trend(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct_return.csv\",\n",
    "    size_scale: float = 600,\n",
    "    alpha: float = 0.4\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot SPD Percentile vs forward returns (30d, 60d, 365d),\n",
    "    where point size is scaled by allocation weight.\n",
    "    Each plot also includes a linear regression trend line.\n",
    "    Y-axis is shown in symlog scale to better visualize returns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert columns to numeric\n",
    "    df[\"spd_percentile\"] = pd.to_numeric(df[\"spd_percentile\"], errors=\"coerce\")\n",
    "    df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\")\n",
    "    df[\"30d_return\"] = pd.to_numeric(df[\"30d_return\"], errors=\"coerce\")\n",
    "    df[\"60d_return\"] = pd.to_numeric(df[\"60d_return\"], errors=\"coerce\")\n",
    "    df[\"365d_return\"] = pd.to_numeric(df[\"365d_return\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df = df.dropna(\n",
    "        subset=[\"spd_percentile\", \"weight\", \"30d_return\", \"60d_return\", \"365d_return\"]\n",
    "    )\n",
    "\n",
    "    # Scale marker size by weight\n",
    "    w = df[\"weight\"].values\n",
    "    w_scaled = (w / (w.max() + 1e-12)) * size_scale\n",
    "\n",
    "    # X-axis values\n",
    "    x = df[\"spd_percentile\"].values\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharex=True)\n",
    "\n",
    "    # -------- 30D --------\n",
    "    y30 = df[\"30d_return\"].values\n",
    "    coef30 = np.polyfit(x, y30, 1)\n",
    "    fit30 = np.poly1d(coef30)\n",
    "\n",
    "    axes[0].scatter(x, y30, s=w_scaled, alpha=alpha)\n",
    "    axes[0].plot(x, fit30(x), color=\"red\", linestyle=\"--\")\n",
    "    axes[0].set_title(\"SPD Percentile vs 30D Return (size = weight)\")\n",
    "    axes[0].set_xlabel(\"SPD Percentile\")\n",
    "    axes[0].set_ylabel(\"30D Return (symlog scale)\")\n",
    "    axes[0].set_yscale(\"symlog\")\n",
    "\n",
    "    # -------- 60D --------\n",
    "    y60 = df[\"60d_return\"].values\n",
    "    coef60 = np.polyfit(x, y60, 1)\n",
    "    fit60 = np.poly1d(coef60)\n",
    "\n",
    "    axes[1].scatter(x, y60, s=w_scaled, alpha=alpha)\n",
    "    axes[1].plot(x, fit60(x), color=\"red\", linestyle=\"--\")\n",
    "    axes[1].set_title(\"SPD Percentile vs 60D Return (size = weight)\")\n",
    "    axes[1].set_xlabel(\"SPD Percentile\")\n",
    "    axes[1].set_ylabel(\"60D Return (symlog scale)\")\n",
    "    axes[1].set_yscale(\"symlog\")\n",
    "\n",
    "    # -------- 365D --------\n",
    "    y365 = df[\"365d_return\"].values\n",
    "    coef365 = np.polyfit(x, y365, 1)\n",
    "    fit365 = np.poly1d(coef365)\n",
    "\n",
    "    axes[2].scatter(x, y365, s=w_scaled, alpha=alpha)\n",
    "    axes[2].plot(x, fit365(x), color=\"red\", linestyle=\"--\", linewidth=0.2)\n",
    "    axes[2].set_title(\"SPD Percentile vs 365D Return (size = weight)\")\n",
    "    axes[2].set_xlabel(\"SPD Percentile\")\n",
    "    axes[2].set_ylabel(\"365D Return (symlog scale)\")\n",
    "    axes[2].set_yscale(\"symlog\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa8cf",
   "metadata": {},
   "source": [
    "# 2.3 Investment Efficiency: Dynamic vs Uniform DCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72cf3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dynamic_vs_uniform_efficiency(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct.csv\",\n",
    "    date_col: str = \"date\",\n",
    "    price_col: str = \"PriceUSD\",\n",
    "    dyn_weight_col: str = \"weight\",\n",
    "    spd_pct_col: str = \"spd_percentile\",\n",
    "    budget: float = 10_000.0,\n",
    "    cheap_pct_threshold: float = 30.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare Dynamic DCA vs Uniform DCA efficiency.\n",
    "\n",
    "    Metrics included:\n",
    "    - Total BTC Accumulation\n",
    "    - Weighted Average SPD (sats per dollar)\n",
    "    - Effective Average Purchase Price ($/BTC)\n",
    "    - Timing Efficiency (% of capital invested in the cheapest X% of SPD)\n",
    "\n",
    "    Required columns in the CSV:\n",
    "    - date (optional but recommended)\n",
    "    - price               -> BTC price\n",
    "    - dynamic_weight      -> Dynamic DCA daily weight\n",
    "    - spd_percentile      -> SPD percentile (0–100)\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load and sort data\n",
    "    # -----------------------------\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df = df.sort_values(date_col)\n",
    "\n",
    "    price = df[price_col].astype(float).values\n",
    "    dyn_w_raw = df[dyn_weight_col].astype(float).values\n",
    "    spd_pct = df[spd_pct_col].astype(float).values\n",
    "\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"DataFrame is empty.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Normalize weights\n",
    "    # -----------------------------\n",
    "    # Dynamic weights may not sum to 1 → normalize\n",
    "    dyn_w = dyn_w_raw / dyn_w_raw.sum()\n",
    "\n",
    "    # Uniform DCA: equal allocation every day\n",
    "    uni_w = np.full(n, 1.0 / n)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute SPD (satoshis per dollar)\n",
    "    # -----------------------------\n",
    "    spd = 100_000_000.0 / price  # sats per $1\n",
    "\n",
    "    # -----------------------------\n",
    "    # Daily allocation & BTC purchased\n",
    "    # -----------------------------\n",
    "    dyn_alloc = dyn_w * budget\n",
    "    uni_alloc = uni_w * budget\n",
    "\n",
    "    dyn_btc = dyn_alloc / price\n",
    "    uni_btc = uni_alloc / price\n",
    "\n",
    "    dyn_total_btc = dyn_btc.sum()\n",
    "    uni_total_btc = uni_btc.sum()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Weighted Average SPD\n",
    "    # -----------------------------\n",
    "    dyn_wspd = np.average(spd, weights=dyn_alloc)\n",
    "    uni_wspd = np.average(spd, weights=uni_alloc)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Effective Average Purchase Price\n",
    "    # -----------------------------\n",
    "    dyn_eff_price = budget / dyn_total_btc\n",
    "    uni_eff_price = budget / uni_total_btc\n",
    "\n",
    "    # -----------------------------\n",
    "    # Timing Efficiency\n",
    "    # % of capital invested in the cheapest X% of prices (SPD percentile)\n",
    "    # -----------------------------\n",
    "    cheap_mask = spd_pct >= cheap_pct_threshold\n",
    "\n",
    "    dyn_timing = dyn_alloc[cheap_mask].sum() / dyn_alloc.sum()\n",
    "    uni_timing = uni_alloc[cheap_mask].sum() / uni_alloc.sum()\n",
    "\n",
    "    dyn_timing_pct = dyn_timing * 100.0\n",
    "    uni_timing_pct = uni_timing * 100.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Summary table\n",
    "    # -----------------------------\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"Dynamic DCA\": {\n",
    "                \"Total BTC Accumulation\": round(dyn_total_btc, 1),\n",
    "                \"Weighted Avg SPD (sats per $)\": round(dyn_wspd, 1),\n",
    "                \"Effective Avg Purchase Price ($/BTC)\": round(dyn_eff_price, 1),\n",
    "                f\"Timing Efficiency (% capital in top {int(cheap_pct_threshold)}%)\": round(dyn_timing_pct, 1),\n",
    "            },\n",
    "            \"Uniform DCA\": {\n",
    "                \"Total BTC Accumulation\": round(uni_total_btc, 1),\n",
    "                \"Weighted Avg SPD (sats per $)\": round(uni_wspd, 1),\n",
    "                \"Effective Avg Purchase Price ($/BTC)\": round(uni_eff_price, 1),\n",
    "                f\"Timing Efficiency (% capital in top {int(cheap_pct_threshold)}%)\": round(uni_timing_pct, 1),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b15b6",
   "metadata": {},
   "source": [
    "# Limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5866d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weighted_return_table_by_horizon(\n",
    "    file_path: str = \"data/weight_price_ma20_spd_pct_return.csv\",\n",
    "    horizon: int = 30,\n",
    "    pct_col: str = \"spd_percentile\",\n",
    "    weight_col: str = \"weight\",\n",
    "    bins = (0, 20, 40, 60, 80, 100),\n",
    "):\n",
    "    \"\"\"\n",
    "    Build ONE return table for a given horizon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to CSV file.\n",
    "    horizon : int\n",
    "        Choose from 30, 60, or 365.\n",
    "    pct_col : str\n",
    "        Column name for SPD percentile.\n",
    "    weight_col : str\n",
    "        Column name for allocation weight.\n",
    "    bins : tuple\n",
    "        Percentile bins.\n",
    "\n",
    "    Required columns in CSV:\n",
    "    - SPD_percentile\n",
    "    - weight\n",
    "    - {horizon}d_return   (e.g., 30d_return, 60d_return, 365d_return)\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    return_col = f\"{horizon}d_return\"\n",
    "    if return_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{return_col}' not found in the dataframe.\")\n",
    "\n",
    "    # Create percentile buckets\n",
    "    labels = [f\"{bins[i]}–{bins[i+1]}%\" for i in range(len(bins) - 1)]\n",
    "    df[\"pct_bucket\"] = pd.cut(df[pct_col], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for bucket, g in df.groupby(\"pct_bucket\"):\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        row = {\"Percentile bucket\": str(bucket)}\n",
    "\n",
    "        # Unweighted average\n",
    "        row[f\"Avg {return_col}\"] = g[return_col].mean()\n",
    "\n",
    "        # Weighted average\n",
    "        w = g[weight_col]\n",
    "        if w.sum() == 0:\n",
    "            row[f\"Weighted Avg {return_col}\"] = np.nan\n",
    "        else:\n",
    "            row[f\"Weighted Avg {return_col}\"] = np.average(g[return_col], weights=w)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    result = pd.DataFrame(rows)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eff6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
